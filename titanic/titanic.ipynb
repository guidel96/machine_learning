{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Environment\" data-toc-modified-id=\"Environment-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Environment</a></span></li><li><span><a href=\"#Importing-Data\" data-toc-modified-id=\"Importing-Data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Importing Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Variables:\" data-toc-modified-id=\"Variables:-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Variables:</a></span></li></ul></li><li><span><a href=\"#Exploratory-Data-Analysis\" data-toc-modified-id=\"Exploratory-Data-Analysis-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Exploratory Data Analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Setting-Data-Types\" data-toc-modified-id=\"Setting-Data-Types-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Setting Data Types</a></span><ul class=\"toc-item\"><li><span><a href=\"#Defining-unique-data-types\" data-toc-modified-id=\"Defining-unique-data-types-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>Defining unique data types</a></span></li><li><span><a href=\"#Defining-categorical-variables\" data-toc-modified-id=\"Defining-categorical-variables-3.1.2\"><span class=\"toc-item-num\">3.1.2&nbsp;&nbsp;</span>Defining categorical variables</a></span></li></ul></li><li><span><a href=\"#Univariate-Descriptive-Analysis\" data-toc-modified-id=\"Univariate-Descriptive-Analysis-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Univariate Descriptive Analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Training-Data---Descriptive-Statistics\" data-toc-modified-id=\"Training-Data---Descriptive-Statistics-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>Training Data - Descriptive Statistics</a></span></li><li><span><a href=\"#Test-Data---Descriptive-Statistics\" data-toc-modified-id=\"Test-Data---Descriptive-Statistics-3.2.2\"><span class=\"toc-item-num\">3.2.2&nbsp;&nbsp;</span>Test Data - Descriptive Statistics</a></span></li></ul></li></ul></li><li><span><a href=\"#Data-Preprocessing\" data-toc-modified-id=\"Data-Preprocessing-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Data Preprocessing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Missing-value-(null/NA)-imputation\" data-toc-modified-id=\"Missing-value-(null/NA)-imputation-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Missing value (null/NA) imputation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Age\" data-toc-modified-id=\"Age-4.1.1\"><span class=\"toc-item-num\">4.1.1&nbsp;&nbsp;</span>Age</a></span></li><li><span><a href=\"#Cabin\" data-toc-modified-id=\"Cabin-4.1.2\"><span class=\"toc-item-num\">4.1.2&nbsp;&nbsp;</span>Cabin</a></span></li><li><span><a href=\"#Embarked\" data-toc-modified-id=\"Embarked-4.1.3\"><span class=\"toc-item-num\">4.1.3&nbsp;&nbsp;</span>Embarked</a></span></li><li><span><a href=\"#Test-Set---Fare\" data-toc-modified-id=\"Test-Set---Fare-4.1.4\"><span class=\"toc-item-num\">4.1.4&nbsp;&nbsp;</span>Test Set - Fare</a></span></li><li><span><a href=\"#Summary\" data-toc-modified-id=\"Summary-4.1.5\"><span class=\"toc-item-num\">4.1.5&nbsp;&nbsp;</span>Summary</a></span></li><li><span><a href=\"#Imputation\" data-toc-modified-id=\"Imputation-4.1.6\"><span class=\"toc-item-num\">4.1.6&nbsp;&nbsp;</span>Imputation</a></span></li></ul></li></ul></li><li><span><a href=\"#Feature-Engineering\" data-toc-modified-id=\"Feature-Engineering-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Feature Engineering</a></span><ul class=\"toc-item\"><li><span><a href=\"#Factorizing-variables\" data-toc-modified-id=\"Factorizing-variables-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Factorizing variables</a></span></li><li><span><a href=\"#Variable-transformations\" data-toc-modified-id=\"Variable-transformations-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Variable transformations</a></span></li><li><span><a href=\"#Outliers\" data-toc-modified-id=\"Outliers-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Outliers</a></span></li><li><span><a href=\"#Scaling-the-data\" data-toc-modified-id=\"Scaling-the-data-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Scaling the data</a></span></li></ul></li><li><span><a href=\"#Feature-Selection\" data-toc-modified-id=\"Feature-Selection-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Feature Selection</a></span><ul class=\"toc-item\"><li><span><a href=\"#Association-between-categorical-variables\" data-toc-modified-id=\"Association-between-categorical-variables-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Association between categorical variables</a></span><ul class=\"toc-item\"><li><span><a href=\"#Categorical-variables:-Cramer's-V-dependencies\" data-toc-modified-id=\"Categorical-variables:-Cramer's-V-dependencies-6.1.1\"><span class=\"toc-item-num\">6.1.1&nbsp;&nbsp;</span>Categorical variables: Cramer's V dependencies</a></span><ul class=\"toc-item\"><li><span><a href=\"#Observations\" data-toc-modified-id=\"Observations-6.1.1.1\"><span class=\"toc-item-num\">6.1.1.1&nbsp;&nbsp;</span>Observations</a></span></li></ul></li><li><span><a href=\"#Categorical-variables:-adapted-mRMR-(NOT-DONE-YET)\" data-toc-modified-id=\"Categorical-variables:-adapted-mRMR-(NOT-DONE-YET)-6.1.2\"><span class=\"toc-item-num\">6.1.2&nbsp;&nbsp;</span>Categorical variables: adapted mRMR (NOT DONE YET)</a></span></li></ul></li><li><span><a href=\"#Association-between-continuous-variables:-Pearson-correlations\" data-toc-modified-id=\"Association-between-continuous-variables:-Pearson-correlations-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Association between continuous variables: Pearson correlations</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Observations:\" data-toc-modified-id=\"Observations:-6.2.0.1\"><span class=\"toc-item-num\">6.2.0.1&nbsp;&nbsp;</span>Observations:</a></span></li></ul></li></ul></li><li><span><a href=\"#Feature-Selection:-Summary\" data-toc-modified-id=\"Feature-Selection:-Summary-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>Feature Selection: Summary</a></span></li></ul></li><li><span><a href=\"#Model-Selection\" data-toc-modified-id=\"Model-Selection-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Model Selection</a></span><ul class=\"toc-item\"><li><span><a href=\"#Procedure-used-to-validate-a-model-&amp;-its-hyperparameters-(REREAD)\" data-toc-modified-id=\"Procedure-used-to-validate-a-model-&amp;-its-hyperparameters-(REREAD)-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Procedure used to validate a model &amp; its hyperparameters (REREAD)</a></span></li><li><span><a href=\"#Supervised-learning\" data-toc-modified-id=\"Supervised-learning-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Supervised learning</a></span></li><li><span><a href=\"#Classification\" data-toc-modified-id=\"Classification-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;</span>Classification</a></span><ul class=\"toc-item\"><li><span><a href=\"#Metrics\" data-toc-modified-id=\"Metrics-7.3.1\"><span class=\"toc-item-num\">7.3.1&nbsp;&nbsp;</span>Metrics</a></span><ul class=\"toc-item\"><li><span><a href=\"#Confusion-Matrix\" data-toc-modified-id=\"Confusion-Matrix-7.3.1.1\"><span class=\"toc-item-num\">7.3.1.1&nbsp;&nbsp;</span>Confusion Matrix</a></span><ul class=\"toc-item\"><li><span><a href=\"#Accuracy\" data-toc-modified-id=\"Accuracy-7.3.1.1.1\"><span class=\"toc-item-num\">7.3.1.1.1&nbsp;&nbsp;</span>Accuracy</a></span></li><li><span><a href=\"#Misclassification-Rate\" data-toc-modified-id=\"Misclassification-Rate-7.3.1.1.2\"><span class=\"toc-item-num\">7.3.1.1.2&nbsp;&nbsp;</span>Misclassification Rate</a></span></li><li><span><a href=\"#Balanced-Error-Rate-(BER)\" data-toc-modified-id=\"Balanced-Error-Rate-(BER)-7.3.1.1.3\"><span class=\"toc-item-num\">7.3.1.1.3&nbsp;&nbsp;</span>Balanced Error Rate (BER)</a></span></li><li><span><a href=\"#Sensitivity-and-specificity\" data-toc-modified-id=\"Sensitivity-and-specificity-7.3.1.1.4\"><span class=\"toc-item-num\">7.3.1.1.4&nbsp;&nbsp;</span>Sensitivity and specificity</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Model-1:-Logistic-Regression\" data-toc-modified-id=\"Model-1:-Logistic-Regression-7.4\"><span class=\"toc-item-num\">7.4&nbsp;&nbsp;</span>Model 1: Logistic Regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#Linear-Regression\" data-toc-modified-id=\"Linear-Regression-7.4.1\"><span class=\"toc-item-num\">7.4.1&nbsp;&nbsp;</span>Linear Regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#Example:-univariate-linear-regression\" data-toc-modified-id=\"Example:-univariate-linear-regression-7.4.1.1\"><span class=\"toc-item-num\">7.4.1.1&nbsp;&nbsp;</span>Example: univariate linear regression</a></span></li><li><span><a href=\"#Estimation-of-the-parameters\" data-toc-modified-id=\"Estimation-of-the-parameters-7.4.1.2\"><span class=\"toc-item-num\">7.4.1.2&nbsp;&nbsp;</span>Estimation of the parameters</a></span><ul class=\"toc-item\"><li><span><a href=\"#Estimation-by-Least-Squares-(OLS)\" data-toc-modified-id=\"Estimation-by-Least-Squares-(OLS)-7.4.1.2.1\"><span class=\"toc-item-num\">7.4.1.2.1&nbsp;&nbsp;</span>Estimation by Least Squares (OLS)</a></span></li><li><span><a href=\"#Properties-of-the-Least-Squares-estimate-$\\hat{\\beta}$\" data-toc-modified-id=\"Properties-of-the-Least-Squares-estimate-$\\hat{\\beta}$-7.4.1.2.2\"><span class=\"toc-item-num\">7.4.1.2.2&nbsp;&nbsp;</span>Properties of the Least Squares estimate $\\hat{\\beta}$</a></span></li><li><span><a href=\"#Variance-of-the-prediction-and-confidence-intervals\" data-toc-modified-id=\"Variance-of-the-prediction-and-confidence-intervals-7.4.1.2.3\"><span class=\"toc-item-num\">7.4.1.2.3&nbsp;&nbsp;</span>Variance of the prediction and confidence intervals</a></span></li><li><span><a href=\"#Evaluating-the-prediction:-Generalization-error-of-the-linear-model\" data-toc-modified-id=\"Evaluating-the-prediction:-Generalization-error-of-the-linear-model-7.4.1.2.4\"><span class=\"toc-item-num\">7.4.1.2.4&nbsp;&nbsp;</span>Evaluating the prediction: Generalization error of the linear model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Mean-Squared-Error-MSE\" data-toc-modified-id=\"Mean-Squared-Error-MSE-7.4.1.2.4.1\"><span class=\"toc-item-num\">7.4.1.2.4.1&nbsp;&nbsp;</span>Mean Squared Error MSE</a></span></li><li><span><a href=\"#The-expected-empirical-error,-a-biased-estimate-of-the-MISE\" data-toc-modified-id=\"The-expected-empirical-error,-a-biased-estimate-of-the-MISE-7.4.1.2.4.2\"><span class=\"toc-item-num\">7.4.1.2.4.2&nbsp;&nbsp;</span>The expected empirical error, a biased estimate of the MISE</a></span></li><li><span><a href=\"#Predicted-Square-Error-(PSE)-and-Final-Prediction-Error-(FPE)\" data-toc-modified-id=\"Predicted-Square-Error-(PSE)-and-Final-Prediction-Error-(FPE)-7.4.1.2.4.3\"><span class=\"toc-item-num\">7.4.1.2.4.3&nbsp;&nbsp;</span>Predicted Square Error (PSE) and Final Prediction Error (FPE)</a></span></li><li><span><a href=\"#Goodness-of-fit:-Squared-Residuals-($R^2$)\" data-toc-modified-id=\"Goodness-of-fit:-Squared-Residuals-($R^2$)-7.4.1.2.4.4\"><span class=\"toc-item-num\">7.4.1.2.4.4&nbsp;&nbsp;</span>Goodness of fit: Squared Residuals ($R^2$)</a></span></li></ul></li><li><span><a href=\"#Example:-Estimating-univariate-linear-regression's-$\\beta$-by-Least-Squares-(OLS)\" data-toc-modified-id=\"Example:-Estimating-univariate-linear-regression's-$\\beta$-by-Least-Squares-(OLS)-7.4.1.2.5\"><span class=\"toc-item-num\">7.4.1.2.5&nbsp;&nbsp;</span>Example: Estimating univariate linear regression's $\\beta$ by Least Squares (OLS)</a></span></li></ul></li><li><span><a href=\"#Properties-of-the-estimator\" data-toc-modified-id=\"Properties-of-the-estimator-7.4.1.3\"><span class=\"toc-item-num\">7.4.1.3&nbsp;&nbsp;</span>Properties of the estimator</a></span></li><li><span><a href=\"#Partitioning-the-variability\" data-toc-modified-id=\"Partitioning-the-variability-7.4.1.4\"><span class=\"toc-item-num\">7.4.1.4&nbsp;&nbsp;</span>Partitioning the variability</a></span></li></ul></li><li><span><a href=\"#Logistic-Regression\" data-toc-modified-id=\"Logistic-Regression-7.4.2\"><span class=\"toc-item-num\">7.4.2&nbsp;&nbsp;</span>Logistic Regression</a></span></li></ul></li><li><span><a href=\"#Decision-Tree-Classifier---Sklearn\" data-toc-modified-id=\"Decision-Tree-Classifier---Sklearn-7.5\"><span class=\"toc-item-num\">7.5&nbsp;&nbsp;</span>Decision Tree Classifier - Sklearn</a></span><ul class=\"toc-item\"><li><span><a href=\"#Model-structure\" data-toc-modified-id=\"Model-structure-7.5.1\"><span class=\"toc-item-num\">7.5.1&nbsp;&nbsp;</span>Model structure</a></span></li><li><span><a href=\"#Learning-procedure\" data-toc-modified-id=\"Learning-procedure-7.5.2\"><span class=\"toc-item-num\">7.5.2&nbsp;&nbsp;</span>Learning procedure</a></span><ul class=\"toc-item\"><li><span><a href=\"#Decision-Tree-Hyperparameters\" data-toc-modified-id=\"Decision-Tree-Hyperparameters-7.5.2.1\"><span class=\"toc-item-num\">7.5.2.1&nbsp;&nbsp;</span>Decision Tree Hyperparameters</a></span></li><li><span><a href=\"#Decision-Trees-for-Regression\" data-toc-modified-id=\"Decision-Trees-for-Regression-7.5.2.2\"><span class=\"toc-item-num\">7.5.2.2&nbsp;&nbsp;</span>Decision Trees for Regression</a></span></li></ul></li><li><span><a href=\"#Decision-Tree-tuning\" data-toc-modified-id=\"Decision-Tree-tuning-7.5.3\"><span class=\"toc-item-num\">7.5.3&nbsp;&nbsp;</span>Decision Tree tuning</a></span></li></ul></li><li><span><a href=\"#Random-Forest\" data-toc-modified-id=\"Random-Forest-7.6\"><span class=\"toc-item-num\">7.6&nbsp;&nbsp;</span>Random Forest</a></span><ul class=\"toc-item\"><li><span><a href=\"#Theory\" data-toc-modified-id=\"Theory-7.6.1\"><span class=\"toc-item-num\">7.6.1&nbsp;&nbsp;</span>Theory</a></span></li><li><span><a href=\"#Basic-model-(no-tuning)\" data-toc-modified-id=\"Basic-model-(no-tuning)-7.6.2\"><span class=\"toc-item-num\">7.6.2&nbsp;&nbsp;</span>Basic model (no tuning)</a></span></li><li><span><a href=\"#Hyperparameters-tuning\" data-toc-modified-id=\"Hyperparameters-tuning-7.6.3\"><span class=\"toc-item-num\">7.6.3&nbsp;&nbsp;</span>Hyperparameters tuning</a></span></li></ul></li><li><span><a href=\"#Logistic-Regression:-OK\" data-toc-modified-id=\"Logistic-Regression:-OK-7.7\"><span class=\"toc-item-num\">7.7&nbsp;&nbsp;</span>Logistic Regression: OK</a></span></li><li><span><a href=\"#Decision-Trees:-OK-needs-tuning\" data-toc-modified-id=\"Decision-Trees:-OK-needs-tuning-7.8\"><span class=\"toc-item-num\">7.8&nbsp;&nbsp;</span>Decision Trees: OK needs tuning</a></span></li><li><span><a href=\"#Support-Vector-Machines-(SVM)\" data-toc-modified-id=\"Support-Vector-Machines-(SVM)-7.9\"><span class=\"toc-item-num\">7.9&nbsp;&nbsp;</span>Support Vector Machines (SVM)</a></span></li><li><span><a href=\"#GBM\" data-toc-modified-id=\"GBM-7.10\"><span class=\"toc-item-num\">7.10&nbsp;&nbsp;</span>GBM</a></span></li><li><span><a href=\"#XXX\" data-toc-modified-id=\"XXX-7.11\"><span class=\"toc-item-num\">7.11&nbsp;&nbsp;</span>XXX</a></span><ul class=\"toc-item\"><li><span><a href=\"#Summary\" data-toc-modified-id=\"Summary-7.11.1\"><span class=\"toc-item-num\">7.11.1&nbsp;&nbsp;</span>Summary</a></span></li></ul></li><li><span><a href=\"#Multivariate-Descriptive-Analysis:-studying-Interdependencies\" data-toc-modified-id=\"Multivariate-Descriptive-Analysis:-studying-Interdependencies-7.12\"><span class=\"toc-item-num\">7.12&nbsp;&nbsp;</span>Multivariate Descriptive Analysis: studying Interdependencies</a></span><ul class=\"toc-item\"><li><span><a href=\"#Correlations-Cat(Y)---Cat(X)\" data-toc-modified-id=\"Correlations-Cat(Y)---Cat(X)-7.12.1\"><span class=\"toc-item-num\">7.12.1&nbsp;&nbsp;</span>Correlations Cat(Y) - Cat(X)</a></span></li><li><span><a href=\"#\" data-toc-modified-id=\"-7.12.2\"><span class=\"toc-item-num\">7.12.2&nbsp;&nbsp;</span></a></span></li></ul></li></ul></li><li><span><a href=\"#Algorithms\" data-toc-modified-id=\"Algorithms-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Algorithms</a></span><ul class=\"toc-item\"><li><span><a href=\"#Defining-factors-(for-categorical-variables)\" data-toc-modified-id=\"Defining-factors-(for-categorical-variables)-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>Defining factors (for categorical variables)</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-19T16:51:47.210165Z",
     "iopub.status.busy": "2022-01-19T16:51:47.209892Z",
     "iopub.status.idle": "2022-01-19T16:51:48.196055Z",
     "shell.execute_reply": "2022-01-19T16:51:48.195155Z",
     "shell.execute_reply.started": "2022-01-19T16:51:47.210135Z"
    }
   },
   "outputs": [],
   "source": [
    "#install.packages(pandas)\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.stats as ss\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas version: 1.3.4\n",
      "seaborn version: 0.11.2\n"
     ]
    }
   ],
   "source": [
    "print(f\"pandas version: {pd.__version__}\")\n",
    "print(f\"seaborn version: {sns.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-19T19:44:57.989153Z",
     "iopub.status.busy": "2022-01-19T19:44:57.988825Z",
     "iopub.status.idle": "2022-01-19T19:44:57.995676Z",
     "shell.execute_reply": "2022-01-19T19:44:57.994788Z",
     "shell.execute_reply.started": "2022-01-19T19:44:57.989116Z"
    }
   },
   "outputs": [],
   "source": [
    "# display options\n",
    "# https://towardsdatascience.com/6-pandas-display-options-you-should-memories-84adf8887bc3\n",
    "\n",
    "pd.set_option('display.max_columns', None) # None if we want it unlimited (no truncation)\n",
    "pd.set_option('display.width', None) \n",
    "pd.set_option('display.max_colwidth', 25)\n",
    "pd.set_option(\"expand_frame_repr\", False) # print cols side by side as it's supposed to be\n",
    "\n",
    "#pd.reset_option('display.max_rows')\n",
    "#pd.reset_option('display.max_columns')\n",
    "#pd.reset_option('display.width')\n",
    "#pd.reset_option('display.float_format')\n",
    "#pd.reset_option('display.max_colwidth')\n",
    "\n",
    "#display only once without affecting all displays:\n",
    "#with pd.option_context('display.max_colwidth', None):\n",
    "#    display(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "# HTML(df.to_html()) # viewing the full content of the cells in a Pandas dataframe is to use IPython's display functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-19T16:51:52.193299Z",
     "iopub.status.busy": "2022-01-19T16:51:52.192969Z",
     "iopub.status.idle": "2022-01-19T16:51:52.211245Z",
     "shell.execute_reply": "2022-01-19T16:51:52.210410Z",
     "shell.execute_reply.started": "2022-01-19T16:51:52.193265Z"
    }
   },
   "outputs": [],
   "source": [
    "# KAGGLE NOTEBOOK\n",
    "'''\n",
    "gender_submission = pd.read_csv(\"../input/titanic/gender_submission.csv\") #load files from \"../input/titanic/train.csv\"\n",
    "#print(gender_submission.head)\n",
    "\n",
    "train_set = pd.read_csv(\"../input/titanic/train.csv\") #load files from \"../input/titanic/train.csv\"\n",
    "#print(train_set.head)\n",
    "\n",
    "test_set = pd.read_csv(\"../input/titanic/test.csv\")\n",
    "#print(test_set.head)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-19T17:09:27.338462Z",
     "iopub.status.busy": "2022-01-19T17:09:27.338033Z",
     "iopub.status.idle": "2022-01-19T17:09:27.352034Z",
     "shell.execute_reply": "2022-01-19T17:09:27.351053Z",
     "shell.execute_reply.started": "2022-01-19T17:09:27.338421Z"
    }
   },
   "outputs": [],
   "source": [
    "# LOCAL NOTEBOOK\n",
    "\n",
    "train_set = pd.read_csv(\"./train.csv\") #load files from \"../input/titanic/train.csv\"\n",
    "#print(train_set.head)\n",
    "\n",
    "test_set = pd.read_csv(\"./test.csv\")\n",
    "#print(test_set.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-19T17:09:31.086608Z",
     "iopub.status.busy": "2022-01-19T17:09:31.086169Z",
     "iopub.status.idle": "2022-01-19T17:09:31.093395Z",
     "shell.execute_reply": "2022-01-19T17:09:31.092398Z",
     "shell.execute_reply.started": "2022-01-19T17:09:31.086567Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set columns: ['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\n",
      "test set columns: ['PassengerId', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\n"
     ]
    }
   ],
   "source": [
    "print(f\"training set columns: {list(train_set.columns)}\")\n",
    "print(f\"test set columns: {list(test_set.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that test and train sets have the same columns except the \"Survived\" column that is only present in the training set. Which is the column that we will want to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables:\n",
    "\n",
    "|             Variable         | Definition    | Key                                          | Type |\n",
    "|------------------------------|---------------|----------------------------------------------|------|\n",
    "| Survived | Y variable. Survival     | 0 = No, 1 = Yes            | categorical |\n",
    "| Pclass   | Ticket class | 1 = 1st, 2 = 2nd, 3 = 3rd  | categorical |\n",
    "| Sex      | Sex          |                            | categorical |\n",
    "| Age      | Age in years |                            | continuous  |\n",
    "| Sibsp    | # of siblings / spouses aboard the Titanic | | continuous |\n",
    "| Parch    | # of parents / children aboard the Titanic | | continuous |\n",
    "| Ticket   | Ticket number   | | categorical ? unique... |\n",
    "| Fare     | Passenger fare  | | continuous |\n",
    "| Cabin    | Cabin number    | | categorical ? unique... |\n",
    "| Embarked | Port of Embarkation | C = Cherbourg, Q = Queenstown, S = Southampton | categorical |\n",
    "\n",
    "For cabin, it might be better to create a new variable referring to the department in which the cabin is located rather than the cabin itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-19T17:09:39.243775Z",
     "iopub.status.busy": "2022-01-19T17:09:39.243276Z",
     "iopub.status.idle": "2022-01-19T17:09:39.257319Z",
     "shell.execute_reply": "2022-01-19T17:09:39.256404Z",
     "shell.execute_reply.started": "2022-01-19T17:09:39.243733Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(train_set.info())  #pandas library has a nice .info() function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Data Types\n",
    "Here the goal is to cast the columns in their corresponding data type to avoid having multiple data types for a same column.\n",
    "The columns that have 'object' as Dtype are the ones we want to uniformize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining unique data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-19T17:09:46.337413Z",
     "iopub.status.busy": "2022-01-19T17:09:46.337001Z",
     "iopub.status.idle": "2022-01-19T17:09:46.352990Z",
     "shell.execute_reply": "2022-01-19T17:09:46.352000Z",
     "shell.execute_reply.started": "2022-01-19T17:09:46.337382Z"
    }
   },
   "outputs": [],
   "source": [
    "# TRAIN SET\n",
    "train_set[\"Name\"] = train_set[\"Name\"].astype(pd.StringDtype())\n",
    "train_set[\"Sex\"] = train_set[\"Sex\"].astype(pd.StringDtype())\n",
    "train_set[\"Ticket\"] = train_set[\"Ticket\"].astype(pd.StringDtype()) #possible to separate the text from the values... but let's keep it as string for now\n",
    "train_set[\"Cabin\"] = train_set[\"Cabin\"].astype(pd.StringDtype()) #possible to separate the text from the values... but let's keep it as string for now\n",
    "train_set[\"Embarked\"] = train_set[\"Embarked\"].astype(pd.StringDtype())\n",
    "\n",
    "# TEST SET\n",
    "test_set[\"Name\"] = test_set[\"Name\"].astype(pd.StringDtype())\n",
    "test_set[\"Sex\"] = test_set[\"Sex\"].astype(pd.StringDtype())\n",
    "test_set[\"Ticket\"] = test_set[\"Ticket\"].astype(pd.StringDtype()) #possible to separate the text from the values... but let's keep it as string for now\n",
    "test_set[\"Cabin\"] = test_set[\"Cabin\"].astype(pd.StringDtype()) #possible to separate the text from the values... but let's keep it as string for now\n",
    "test_set[\"Embarked\"] = test_set[\"Embarked\"].astype(pd.StringDtype())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining categorical variables\n",
    "Amongst the categorical variables, it is important to make a distinction between ordered and unordered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-30T20:29:05.741806Z",
     "iopub.status.busy": "2021-12-30T20:29:05.741496Z",
     "iopub.status.idle": "2021-12-30T20:29:05.749974Z",
     "shell.execute_reply": "2021-12-30T20:29:05.749234Z",
     "shell.execute_reply.started": "2021-12-30T20:29:05.741774Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://pandas.pydata.org/pandas-docs/stable/user_guide/categorical.html\n",
    "# Panda's Categorical is more or less similar to R's factor data type\n",
    "\n",
    "# UNORDERED CATEGORIES\n",
    "# Categorical data has a specific category dtype:\n",
    "# By converting an existing Series or column to a category dtype: df[\"B\"] = df[\"A\"].astype(\"category\")\n",
    "# By using special functions, such as cut(), which groups data into discrete bins\n",
    "\n",
    "# Analogously, all columns in an existing DataFrame can be batch converted using DataFrame.astype():\n",
    "# df = pd.DataFrame({\"A\": list(\"abca\"), \"B\": list(\"bccd\")})\n",
    "# df_cat = df.astype(\"category\") \n",
    "\n",
    "# ORDERED CATEGORIES\n",
    "# above where we passed dtype='category', we used the default behavior: \n",
    "    # Categories are inferred from the data.    \n",
    "    # Categories are unordered.\n",
    "# To control those behaviors, instead of passing 'category', use an instance of CategoricalDtype.\n",
    "# from pandas.api.types import CategoricalDtype\n",
    "# a CategoricalDtype can be used with a DataFrame to ensure that categories are consistent among all columns.\n",
    "# cat_type = CategoricalDtype(categories=list(\"abcd\"), ordered=True)\n",
    "# df_cat = df.astype(cat_type)\n",
    "\n",
    "# SUMMARY\n",
    "# A categorical’s type is fully described by\n",
    "    # categories: a sequence of unique values and no missing values (The categories argument is optional)\n",
    "    # ordered: a boolean\n",
    "# A CategoricalDtype can be used in any place pandas expects a dtype. For example pandas.read_csv(), pandas.DataFrame.astype(), or in the Series constructor.\n",
    "# Removing categories can be done by using the remove_categories() method\n",
    "# Removing unused categories can also be done: .cat.remove_unused_categories()\n",
    "# Missing values should not be included in the Categorical’s categories, only in the values. Instead, it is understood that NaN is different, and is always a possibility. When working with the Categorical’s codes, missing values will always have a code of -1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, all categorical variables are unordered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-19T17:10:56.201848Z",
     "iopub.status.busy": "2022-01-19T17:10:56.201042Z",
     "iopub.status.idle": "2022-01-19T17:10:56.210950Z",
     "shell.execute_reply": "2022-01-19T17:10:56.210186Z",
     "shell.execute_reply.started": "2022-01-19T17:10:56.201754Z"
    }
   },
   "outputs": [],
   "source": [
    "# TRAIN SET\n",
    "train_set[\"Survived\"] = train_set[\"Survived\"].astype(\"category\")\n",
    "train_set[\"Pclass\"] = train_set[\"Pclass\"].astype(\"category\")\n",
    "train_set[\"Sex\"] = train_set[\"Sex\"].astype(\"category\")\n",
    "train_set[\"Embarked\"] = train_set[\"Embarked\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-19T17:10:57.994350Z",
     "iopub.status.busy": "2022-01-19T17:10:57.993881Z",
     "iopub.status.idle": "2022-01-19T17:10:58.002852Z",
     "shell.execute_reply": "2022-01-19T17:10:58.001907Z",
     "shell.execute_reply.started": "2022-01-19T17:10:57.994279Z"
    }
   },
   "outputs": [],
   "source": [
    "# TEST SET\n",
    "test_set[\"Pclass\"] = test_set[\"Pclass\"].astype(\"category\")\n",
    "test_set[\"Sex\"] = test_set[\"Sex\"].astype(\"category\")\n",
    "test_set[\"Embarked\"] = test_set[\"Embarked\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-19T17:11:00.525474Z",
     "iopub.status.busy": "2022-01-19T17:11:00.525013Z",
     "iopub.status.idle": "2022-01-19T17:11:00.549370Z",
     "shell.execute_reply": "2022-01-19T17:11:00.548505Z",
     "shell.execute_reply.started": "2022-01-19T17:11:00.525438Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype   \n",
      "---  ------       --------------  -----   \n",
      " 0   PassengerId  891 non-null    int64   \n",
      " 1   Survived     891 non-null    category\n",
      " 2   Pclass       891 non-null    category\n",
      " 3   Name         891 non-null    string  \n",
      " 4   Sex          891 non-null    category\n",
      " 5   Age          714 non-null    float64 \n",
      " 6   SibSp        891 non-null    int64   \n",
      " 7   Parch        891 non-null    int64   \n",
      " 8   Ticket       891 non-null    string  \n",
      " 9   Fare         891 non-null    float64 \n",
      " 10  Cabin        204 non-null    string  \n",
      " 11  Embarked     889 non-null    category\n",
      "dtypes: category(4), float64(2), int64(3), string(3)\n",
      "memory usage: 59.8 KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 418 entries, 0 to 417\n",
      "Data columns (total 11 columns):\n",
      " #   Column       Non-Null Count  Dtype   \n",
      "---  ------       --------------  -----   \n",
      " 0   PassengerId  418 non-null    int64   \n",
      " 1   Pclass       418 non-null    category\n",
      " 2   Name         418 non-null    string  \n",
      " 3   Sex          418 non-null    category\n",
      " 4   Age          332 non-null    float64 \n",
      " 5   SibSp        418 non-null    int64   \n",
      " 6   Parch        418 non-null    int64   \n",
      " 7   Ticket       418 non-null    string  \n",
      " 8   Fare         417 non-null    float64 \n",
      " 9   Cabin        91 non-null     string  \n",
      " 10  Embarked     418 non-null    category\n",
      "dtypes: category(3), float64(2), int64(3), string(3)\n",
      "memory usage: 27.9 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# VERIFICATION\n",
    "print(train_set.info())\n",
    "print(test_set.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Descriptive Analysis\n",
    "\n",
    "In this part the goal is to look at each variable individually (univariate analysis) and extract some basic information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data - Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-19T19:08:02.487546Z",
     "iopub.status.busy": "2022-01-19T19:08:02.487262Z",
     "iopub.status.idle": "2022-01-19T19:08:02.558123Z",
     "shell.execute_reply": "2022-01-19T19:08:02.557153Z",
     "shell.execute_reply.started": "2022-01-19T19:08:02.487516Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating 2 summary tables containing important information: one for categorical vars and another one for the other vars\n",
    "\n",
    "# create 2 df's, and define their column names:\n",
    "categorical_cols = pd.DataFrame(columns=['name','dtype', 'nb unique', 'has null values', 'nb null', 'categories'])\n",
    "other_cols = pd.DataFrame(columns=['name','dtype', 'min', 'max', 'nb unique', 'has null values', 'nb null'])\n",
    "\n",
    "for column in train_set.columns:\n",
    "    if (train_set[column].dtype.name != 'category'):          # if the column has NOT dtype = 'category'\n",
    "        \n",
    "        # how weird it may sound, 'other' is the parameter that takes the data to append as input (https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.append.html)\n",
    "        other_cols = other_cols.append(other = {\n",
    "            \"name\":  column,\n",
    "            \"dtype\": train_set[column].dtype,\n",
    "            \"min\": train_set[column].min(),\n",
    "            \"max\": train_set[column].max(),\n",
    "            \"nb unique\": train_set[column].nunique(),                    # number of unique values\n",
    "            \"has null values\": train_set[column].isnull().values.any(),  # will return True if there is any null value\n",
    "            \"nb null\": train_set[column].isnull().sum()\n",
    "        }, ignore_index=True ) \n",
    "    \n",
    "    elif (train_set[column].dtype.name == 'category'):         # if the column has dtype = 'category'\n",
    "        \n",
    "        categorical_cols = categorical_cols.append(other = {\n",
    "            'name': column,\n",
    "            'dtype': train_set[column].dtype, \n",
    "            'nb unique': train_set[column].nunique(),                    # number of unique values (=nb categories)\n",
    "            'has null values': train_set[column].isnull().values.any(),  # will return True if there is any null\n",
    "            'nb null': train_set[column].isnull().sum(),                 # sum of the values that are null\n",
    "            'categories': list(train_set[column].cat.categories.values)  # list(train_set[column].unique()) \n",
    "        }, ignore_index=True)   \n",
    "        \n",
    "# Other useful codes\n",
    "# df_numeric = df.select_dtypes(include=[np.float64,np.int64]) to select specific columns based on dtype\n",
    "# Extracting the unique values of a df (columndf): list(df[column].unique()) OR list(df[column].cat.categories) OR df[column].cat.categories.values (better to use unique() to have only a list!)\n",
    "    # unique() returns categories in the order of appearance, and it only includes values that are actually present\n",
    "    # .cat.categories.values seems much faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-19T19:34:00.269761Z",
     "iopub.status.busy": "2022-01-19T19:34:00.269407Z",
     "iopub.status.idle": "2022-01-19T19:34:00.286070Z",
     "shell.execute_reply": "2022-01-19T19:34:00.285073Z",
     "shell.execute_reply.started": "2022-01-19T19:34:00.269722Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       name     dtype nb unique has null values nb null      categories\n",
      "0  Survived  category         2           False       0          [0, 1]\n",
      "1    Pclass  category         3           False       0       [1, 2, 3]\n",
      "2       Sex  category         2           False       0  [female, male]\n",
      "3  Embarked  category         3            True       2       [C, Q, S]\n",
      "#######\n",
      "          name    dtype                  min                          max  \\\n",
      "0  PassengerId    int64                    1                          891   \n",
      "1         Name   string  Abbing, Mr. Anthony  van Melkebeke, Mr. Philemon   \n",
      "2          Age  float64                 0.42                         80.0   \n",
      "3        SibSp    int64                    0                            8   \n",
      "4        Parch    int64                    0                            6   \n",
      "5       Ticket   string               110152                    WE/P 5735   \n",
      "6         Fare  float64                  0.0                     512.3292   \n",
      "7        Cabin   string                  A10                            T   \n",
      "\n",
      "  nb unique has null values nb null  \n",
      "0       891           False       0  \n",
      "1       891           False       0  \n",
      "2        88            True     177  \n",
      "3         7           False       0  \n",
      "4         7           False       0  \n",
      "5       681           False       0  \n",
      "6       248           False       0  \n",
      "7       147            True     687  \n"
     ]
    }
   ],
   "source": [
    "print(categorical_cols)\n",
    "print(\"#######\")\n",
    "print(other_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation: ```Cabin```, ```Age```, and ```Embarked``` have null values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data - Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating 2 summary tables containing important information: one for categorical vars and another one for the other vars\n",
    "\n",
    "# create 2 df's, and define their column names:\n",
    "categorical_cols = pd.DataFrame(columns=['name','dtype', 'nb unique', 'has null values', 'nb null', 'categories'])\n",
    "other_cols = pd.DataFrame(columns=['name','dtype', 'min', 'max', 'nb unique', 'has null values', 'nb null'])\n",
    "\n",
    "for column in test_set.columns:\n",
    "    if (test_set[column].dtype.name != 'category'):          # if the column has NOT dtype = 'category'\n",
    "        \n",
    "        # how weird it may sound, 'other' is the parameter that takes the data to append as input (https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.append.html)\n",
    "        other_cols = other_cols.append(other = {\n",
    "            \"name\":  column,\n",
    "            \"dtype\": test_set[column].dtype,\n",
    "            \"min\": test_set[column].min(),\n",
    "            \"max\": test_set[column].max(),\n",
    "            \"nb unique\": test_set[column].nunique(),                    # number of unique values\n",
    "            \"has null values\": test_set[column].isnull().values.any(),  # will return True if there is any null value\n",
    "            \"nb null\": test_set[column].isnull().sum()\n",
    "        }, ignore_index=True ) \n",
    "    \n",
    "    elif (test_set[column].dtype.name == 'category'):         # if the column has dtype = 'category'\n",
    "        \n",
    "        categorical_cols = categorical_cols.append(other = {\n",
    "            'name': column,\n",
    "            'dtype': test_set[column].dtype, \n",
    "            'nb unique': test_set[column].nunique(),                    # number of unique values (=nb categories)\n",
    "            'has null values': test_set[column].isnull().values.any(),  # will return True if there is any null\n",
    "            'nb null': test_set[column].isnull().sum(),                 # sum of the values that are null\n",
    "            'categories': list(test_set[column].cat.categories.values)  # list(train_set[column].unique()) \n",
    "        }, ignore_index=True)   \n",
    "        \n",
    "# Other useful codes\n",
    "# df_numeric = df.select_dtypes(include=[np.float64,np.int64]) to select specific columns based on dtype\n",
    "# Extracting the unique values of a df (columndf): list(df[column].unique()) OR list(df[column].cat.categories) OR df[column].cat.categories.values (better to use unique() to have only a list!)\n",
    "    # unique() returns categories in the order of appearance, and it only includes values that are actually present\n",
    "    # .cat.categories.values seems much faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       name     dtype nb unique has null values nb null      categories\n",
      "0    Pclass  category         3           False       0       [1, 2, 3]\n",
      "1       Sex  category         2           False       0  [female, male]\n",
      "2  Embarked  category         3           False       0       [C, Q, S]\n",
      "#######\n",
      "          name    dtype                            min  \\\n",
      "0  PassengerId    int64                            892   \n",
      "1         Name   string  Abbott, Master. Eugene Joseph   \n",
      "2          Age  float64                           0.17   \n",
      "3        SibSp    int64                              0   \n",
      "4        Parch    int64                              0   \n",
      "5       Ticket   string                         110469   \n",
      "6         Fare  float64                            0.0   \n",
      "\n",
      "                                 max nb unique has null values nb null  \n",
      "0                               1309       418           False       0  \n",
      "1  van Billiard, Master. Walter John       418           False       0  \n",
      "2                               76.0        79           False       0  \n",
      "3                                  8         7           False       0  \n",
      "4                                  9         8           False       0  \n",
      "5                        W.E.P. 5734       363           False       0  \n",
      "6                           512.3292       169            True       1  \n"
     ]
    }
   ],
   "source": [
    "print(categorical_cols)\n",
    "print(\"#######\")\n",
    "print(other_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation: ```Fare``` has one null value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "## Missing value (null/NA) imputation\n",
    "\n",
    "It was observed in previous sections that:\n",
    "- **Age:** continuous variable that has 177 null values (20%)\n",
    "- **Cabin:** partly categorical (kind of) variable that has 687 null values (77%)\n",
    "- **Embarked:** categorical variable that has 2 null values\n",
    "\n",
    "In this part we first take a deeper look into these missing values, and then propose a solution to deal with them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-19T20:05:32.395418Z",
     "iopub.status.busy": "2022-01-19T20:05:32.395134Z",
     "iopub.status.idle": "2022-01-19T20:05:32.411701Z",
     "shell.execute_reply": "2022-01-19T20:05:32.410842Z",
     "shell.execute_reply.started": "2022-01-19T20:05:32.395387Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Moran, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330877</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Williams, Mr. Charles Eugene</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>244373</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Masselmani, Mrs. Fatima</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2649</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Emir, Mr. Farred Chehab</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2631</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>O'Dwyer, Miss. Ellen \"Nellie\"</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330959</td>\n",
       "      <td>7.8792</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Todoroff, Mr. Lalio</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>349216</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Spencer, Mrs. William Augustus (Marie Eugenie)</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17569</td>\n",
       "      <td>146.5208</td>\n",
       "      <td>B78</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Glynn, Miss. Mary Agatha</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>335677</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Mamee, Mr. Hanna</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2677</td>\n",
       "      <td>7.2292</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Kraeff, Mr. Theodor</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>349253</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####### LOOK INTO NAs FOR 'AGE':\n",
    "#Look into null values: https://stackoverflow.com/questions/17071871/how-do-i-select-rows-from-a-dataframe-based-on-column-values\n",
    "\n",
    "HTML(train_set[train_set[\"Age\"].isnull()].head(10).to_html()) # select the null values for Age to see what's going on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-19T20:05:21.756564Z",
     "iopub.status.busy": "2022-01-19T20:05:21.755677Z",
     "iopub.status.idle": "2022-01-19T20:05:22.330734Z",
     "shell.execute_reply": "2022-01-19T20:05:22.329729Z",
     "shell.execute_reply.started": "2022-01-19T20:05:21.756523Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#the displot is used to visualize a distribution (https://seaborn.pydata.org/tutorial/distributions.html)\n",
    "print(train_set[\"Age\"][~train_set[\"Age\"].isnull()].median())       #median for non-null values of the Age column\n",
    "ax = sns.displot(x=\"Age\",data=train_set[~train_set[\"Age\"].isnull()]) #the ~ is used to indicate to take ALL EXCEPT the rows matching the condition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's hard to find a pattern and therefore to set an age.\n",
    "Less bad solution seems to be to allocate the unknown age to the median age, which is 28.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cabin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-19T20:06:50.686920Z",
     "iopub.status.busy": "2022-01-19T20:06:50.686285Z",
     "iopub.status.idle": "2022-01-19T20:06:50.697327Z",
     "shell.execute_reply": "2022-01-19T20:06:50.696351Z",
     "shell.execute_reply.started": "2022-01-19T20:06:50.686867Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count         204\n",
      "unique        147\n",
      "top       B96 B98\n",
      "freq            4\n",
      "Name: Cabin, dtype: object\n",
      "nb null: 687\n"
     ]
    }
   ],
   "source": [
    "####### LOOK INTO NAs FOR 'CABIN':\n",
    "\n",
    "print(train_set[\"Cabin\"].describe())\n",
    "print(f\"nb null: \"+str(train_set[\"Cabin\"].isnull().sum()))\n",
    "#print(train_set[\"Cabin\"].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embarked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-19T20:07:03.789214Z",
     "iopub.status.busy": "2022-01-19T20:07:03.788613Z",
     "iopub.status.idle": "2022-01-19T20:07:03.805613Z",
     "shell.execute_reply": "2022-01-19T20:07:03.804388Z",
     "shell.execute_reply.started": "2022-01-19T20:07:03.789175Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count     889\n",
      "unique      3\n",
      "top         S\n",
      "freq      644\n",
      "Name: Embarked, dtype: object\n",
      "nb null: 2\n",
      "     PassengerId Survived Pclass                                       Name  \\\n",
      "61            62        1      1                        Icard, Miss. Amelie   \n",
      "829          830        1      1  Stone, Mrs. George Nelson (Martha Evelyn)   \n",
      "\n",
      "        Sex   Age  SibSp  Parch  Ticket  Fare Cabin Embarked  \n",
      "61   female  38.0      0      0  113572  80.0   B28      NaN  \n",
      "829  female  62.0      0      0  113572  80.0   B28      NaN  \n"
     ]
    }
   ],
   "source": [
    "####### LOOK INTO NAs FOR 'EMBARKED':\n",
    "\n",
    "print(train_set[\"Embarked\"].describe())\n",
    "print(f\"nb null: \"+str(train_set[\"Embarked\"].isnull().sum()))\n",
    "print(train_set.loc[ train_set[\"Embarked\"].isnull(), : ]) #show the rows that have \"Embarked\" as null\n",
    "\n",
    "#print(train_set[\"Cabin\"].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only two people, we can assign them to the most frequent category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Set - Fare\n",
    "\n",
    "One missing value was observed. For simplicity, we will assume the fare to be equal to the mean fare of all the test set.\n",
    "We could do more elaborated stuff but it's not worth it for one observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>1044</td>\n",
       "      <td>3</td>\n",
       "      <td>Storey, Mr. Thomas</td>\n",
       "      <td>male</td>\n",
       "      <td>60.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3701</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId Pclass                Name   Sex   Age  SibSp  Parch Ticket  \\\n",
       "152         1044      3  Storey, Mr. Thomas  male  60.5      0      0   3701   \n",
       "\n",
       "     Fare Embarked  \n",
       "152   NaN        S  "
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.loc[test_set[\"Fare\"].isnull(), : ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe missing values for:  \n",
    "- **\"Age\" (177/891 = 19%):**  \n",
    "Quite a few missing values. Since not possible to infer them, we will **replace them with the median**.\n",
    "\n",
    "- **\"Cabin\" (77% !!!):**  \n",
    "too many missing values and low chance of retrieving the correct value --> **remove the column**\n",
    "Below it appears clear that Cabin has many NULL values (77% of the observations are null). So we can't do anything with this column. Additionally, some passengers have booked several cabins on their name, that is probably why there are so many null values: parents booking cabins for their children etc. It seems very difficult to infer who belongs to what cabin...  \n",
    "- **\"Embarked\" (only 2):**  \n",
    "it's a categorical variable so the rule here is to **assign the missing values to the most frequent category of the column** (reference: [imputation of missing values for categories in Pandas](https://stackoverflow.com/questions/32617811/imputation-of-missing-values-for-categories-in-pandas))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-31T11:42:05.967382Z",
     "iopub.status.busy": "2021-12-31T11:42:05.966267Z",
     "iopub.status.idle": "2021-12-31T11:42:05.971418Z",
     "shell.execute_reply": "2021-12-31T11:42:05.97022Z",
     "shell.execute_reply.started": "2021-12-31T11:42:05.96734Z"
    }
   },
   "outputs": [],
   "source": [
    "#In R randomForest package there is na.roughfix option : A completed data matrix or data frame. \n",
    "#For numeric variables, NAs are replaced with column medians. \n",
    "#For factor variables, NAs are replaced with the most frequent levels (breaking ties at random). \n",
    "#If object contains no NAs, it is returned unaltered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-31T18:24:27.193441Z",
     "iopub.status.busy": "2021-12-31T18:24:27.192454Z",
     "iopub.status.idle": "2021-12-31T18:24:27.200296Z",
     "shell.execute_reply": "2021-12-31T18:24:27.199401Z",
     "shell.execute_reply.started": "2021-12-31T18:24:27.193384Z"
    }
   },
   "outputs": [],
   "source": [
    "########### AGE: REPLACE NULL VALUES WITH MEDIAN\n",
    "# EXAMPLE:\n",
    "# df.loc[df['a'] > 10, ['a','c']] = 1\n",
    "#syntax: \n",
    "    #df.loc[] to select the row\n",
    "    # df['a'] > 10 to select the rows that match the condition\n",
    "    # ['a','c'] to select the columns of which we want to change a value (otherwise it will replace the whole row!)\n",
    "    # = 1 set the new value\n",
    "train_set.loc[train_set[\"Age\"].isnull(), \"Age\"] = train_set[\"Age\"][train_set[\"Age\"].notnull()].median()\n",
    "test_set.loc[test_set[\"Age\"].isnull(), \"Age\"] = test_set[\"Age\"][test_set[\"Age\"].notnull()].median() # could also decide to set the median of the train set ???\n",
    "# Alternative solution: df = df.fillna(df.median()) where df = train_set[\"Age\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-31T18:44:04.140567Z",
     "iopub.status.busy": "2021-12-31T18:44:04.140283Z",
     "iopub.status.idle": "2021-12-31T18:44:04.148927Z",
     "shell.execute_reply": "2021-12-31T18:44:04.148291Z",
     "shell.execute_reply.started": "2021-12-31T18:44:04.140541Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n",
      "       'Parch', 'Ticket', 'Fare', 'Embarked'],\n",
      "      dtype='object')\n",
      "Index(['PassengerId', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch',\n",
      "       'Ticket', 'Fare', 'Embarked'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "########### CABIN: REMOVE COLUMN, TOO MANY NAs\n",
    "# REMOVING 'cabin' column from both test set and training set\n",
    "train_set = train_set.drop(\"Cabin\", axis=1) #axis = 1 refers to column, axis = 0 refers to rows\n",
    "print(train_set.columns)\n",
    "test_set = test_set.drop(\"Cabin\", axis=1) #axis = 1 refers to column, axis = 0 refers to rows\n",
    "print(test_set.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-01T10:32:28.623352Z",
     "iopub.status.busy": "2022-01-01T10:32:28.622963Z",
     "iopub.status.idle": "2022-01-01T10:32:28.718351Z",
     "shell.execute_reply": "2022-01-01T10:32:28.717223Z",
     "shell.execute_reply.started": "2022-01-01T10:32:28.623247Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb null: 0\n",
      "nb null: 0\n"
     ]
    }
   ],
   "source": [
    "########### EMBARKED: ASSIGN NAs TO MOST FREQUENT COLUMN\n",
    "# fill NaNs with the most frequent value from one column: \n",
    "# https://stackoverflow.com/questions/32617811/imputation-of-missing-values-for-categories-in-pandas\n",
    "# df = df.fillna(df.mode().iloc[0])\n",
    "# alternative: col.fillna(col.value_counts().index[0], inplace=True)\n",
    "# HOW TO FIND THE MOST FREQUENT CLASS: \n",
    "     # train_set[\"Embarked\"].mode.iloc[0]\n",
    "     # train_set[\"Embarked\"].value_counts().index[0]\n",
    "# train_set[[\"Embarked\"]] = train_set[[\"Embarked\"]].fillna(train_set[\"Embarked\"].mode.iloc[0])  # outdated\n",
    "# test_set[[\"Embarked\"]] = test_set[[\"Embarked\"]].fillna(test_set[\"Embarked\"].mode.iloc[0])     # outdated\n",
    "\n",
    "train_set[[\"Embarked\"]] = train_set[[\"Embarked\"]].fillna(train_set[\"Embarked\"].value_counts().index[0])\n",
    "test_set[[\"Embarked\"]] = test_set[[\"Embarked\"]].fillna(test_set[\"Embarked\"].value_counts().index[0])\n",
    "\n",
    "# Verification\n",
    "print(f\"nb null: \"+str(train_set[\"Embarked\"].isnull().sum()))\n",
    "print(f\"nb null: \"+str(test_set[\"Embarked\"].isnull().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### FARE: REPLACE NA with median of Pclass=3 \n",
    "\n",
    "# When we want to select only the rows that match a specific CATEGORY\n",
    "#df.loc[df[\"column\"]== a, : ]\n",
    "\n",
    "#test_set.loc[test_set[\"Pclass\"]==3, : ]     # select all columns\n",
    "pclass3_median_fare = test_set.loc[test_set[\"Pclass\"]==3, \"Fare\"].median() # return only the \"Fare\" column, and compute the median\n",
    "\n",
    "\n",
    "test_set.loc[test_set[\"Fare\"].isnull(), \"Fare\"] = pclass3_median_fare # replace null by median of Pclass = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other interesting techniques to fill NAs:\n",
    "# df.fillna({'a':0,'b':3}) #using a dict to reference the value to set for different columns\n",
    "\n",
    "# Filling the columns based on the dtype (to determine whether it's a categorical or continuous variable)\n",
    "# # numeric columns\n",
    "# df.fillna(df.select_dtypes(include='number').mean().iloc[0], inplace=True)\n",
    "\n",
    "# categorical columns\n",
    "# df.fillna(df.select_dtypes(include='object').mode().iloc[0], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "## Factorizing variables\n",
    "Already done at the beginning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable transformations\n",
    "Eventually transforming some variables, e.g.:\n",
    "- reducing number of classes  \n",
    "- transforming continuous var into categorical (such as age groups)  \n",
    "- uniformizing (lowercasing) class names  \n",
    "- recording 'unknown' etc as NULL value   \n",
    "- One hot encoding\n",
    "- ...\n",
    "\n",
    "Nothing particular observed for this dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers\n",
    "Check the range.\n",
    "\n",
    "In the Descriptive Statistics earlier it was observed that the minimum ```Fare``` was 0, which might be possible but would be strange if too many values are concerned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sns.displot(x=\"Fare\",data=train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.loc[train_set[\"Fare\"]==0, \"Fare\"].count() #only 15 who have a Fare = 0, that's OK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling the data\n",
    "\n",
    "It is also very important to standardize the data, as some of the algorithms that will be used later assume that the data has been standardized earlier. Because there are different techniques to scale (e.g. normalization, min-max) some models have specific requirements, we will scale the data when training the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "In this part we propose some selections of features that can be used later on when building the models.\n",
    "In order to select the right features, we need to analyze the dependencies between the different features.\n",
    "Note: this part is not required for some algorithms, as these algorithms do the feature selection themselves (e.g. decision trees decide themselves what's order is best)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Association between categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical variables: Cramer's V dependencies\n",
    "\n",
    "Analyzing the dependency between two categorical variables, say $X_A$ and $X_B$, can be done by conducting a  Chi-squared test. It tests:  \n",
    "\n",
    "$H_0$: $X_A$ and $X_B$ are indepedent  \n",
    "$H_a$: $X_A$ and $X_B$ are not indepedent\n",
    "\n",
    "The idea is to record the different combinations of these two variables in a contingency table. Then, under the null hypothesis, it is expected that the observed values of these combinations are uniformly distributed over the table. If that is not the case, then the null hypothesis is rejected and we cannot assume that $X_A$ and $X_B$ are independent.  \n",
    "\n",
    "$$\\chi^2 = \\sum_i \\sum_j \\frac{(O_{ij}-E_{ij})^2}{E_{ij}} $$\n",
    "\n",
    "with : $df=(n-1)(m-1)$ degrees of freedom, where n and m are the number of classes/levels in the first and second variable respectively.   \n",
    "where:  \n",
    "- $E_{ij}$: is the expected counts under the null hypothesis. \n",
    "- $O_{ij}$: is the observed count in the data. $O_{ij}= \\frac{row \\ j \\ total * column \\ i \\ total}{ table \\ total}$\n",
    "\n",
    "\n",
    "Hence, in practice, a contingency table is made and the null hypothesis implies that the row and the column variables of the contingency table are independent. Under the null hypothesis, the values observed in the contingency table should be similar to the expected ones. The Chi-square test can be conducted in R using ```chisq.test()``` with either the contingency table or the two variables as argument. \n",
    "\n",
    "Some interesting tutorials regarding the implementation of this test can be found at: https://www.datacamp.com/community/tutorials/contingency-analysis-r and https://www.datacamp.com/community/tutorials/contingency-tables-r.\n",
    "\n",
    "\n",
    "Based on the Chi-squared test, we can also compute Cramer's V. This is very interesting to us as it provides a **measure of the intensity of the dependency between the two variables**, which the Chi-squared test doesn't do (**Chi-Square assesses only significance**). The Cramer's coefficient is the following:\n",
    "\n",
    "$$V = \\sqrt{\\frac{\\chi^2}{\\chi^2_{max}}} $$ \n",
    "\n",
    "where: $\\chi^2_\\text{max} = N \\times ( \\min(N, P) - 1 )$, with $N$ the number of rows (i.e. number of classes of the first variable) and $P$ the number of columns (i.e. number of classes of the second variable) in the contingency matrix.\n",
    "\n",
    "Cramer's V is therefore contained in $[0,1]$, where 0 is maximal independence, and 1 reflects a maximal correlation.\n",
    "\n",
    "Note: Another approach to assess the intensity of the dependence could consist of using distance metrics but these methods are scale-sensitive and do not always provide reliable results when the number of classes between the two variables is different, unless performing one-hot encoding.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, we use ```scipy.stats.chi2_contingency(observed, correction=True, lambda_=None)```. This function computes the chi-square statistic and p-value for the hypothesis test of independence of the observed frequencies in the contingency table observed. The expected frequencies are computed based on the marginal sums under the assumption of independence [https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chi2_contingency.html].\n",
    "\n",
    "```ss.chi2_contingency()```takes as input:\n",
    "- observed:    \n",
    "The contingency table. The table contains the observed frequencies (i.e. number of occurrences) in each category. In the two-dimensional case, the table is often described as an “R x C table”.  \n",
    "\n",
    "- correction:   \n",
    "If True, **and** the degrees of freedom is 1, apply Yates’ correction for continuity. The effect of the correction is to adjust each observed value by 0.5 towards the corresponding expected value.\n",
    "\n",
    "and returns:\n",
    "\n",
    "| returned parameter | Description                                                                    |\n",
    "|--------------------|--------------------------------------------------------------------------------|\n",
    "| chi2 (float)       | The test statistic.                                                            |\n",
    "| p (float)          | The p-value of the test                                                        |\n",
    "| dof (int)          | Degrees of freedom                                                             |\n",
    "| expected (ndarray) | The expected frequencies (under H_0), based on the marginal sums of the table. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Cramer's V on categorical vars (X and Y)\n",
    "\n",
    "#import scipy.stats as ss\n",
    "\n",
    "def cramers_v(confusion_matrix):\n",
    "    \"\"\" \n",
    "    calculate Cramers V statistic for one categorial-to-categorial association.\n",
    "    uses correction from Bergsma and Wicher,\n",
    "    Journal of the Korean Statistical Society 42 (2013): 323-328\n",
    "    \"\"\"\n",
    "    if confusion_matrix.shape[0]==2: \n",
    "        correct=False\n",
    "    else:\n",
    "        correct=True\n",
    "\n",
    "    chi2 = ss.chi2_contingency(confusion_matrix, correction=correct)[0] # returns the contingency table\n",
    "    #chi2 = ss.chi2_contingency(confusion_matrix)[0]    # returns the contingency table\n",
    "    n = confusion_matrix.sum()                         # total number of observations\n",
    "    phi2 = chi2 / n                                    # proportions (%)\n",
    "    r, k = confusion_matrix.shape\n",
    "    phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))\n",
    "    rcorr = r - ((r-1)**2)/(n-1)\n",
    "    kcorr = k - ((k-1)**2)/(n-1)\n",
    "    \n",
    "    return np.sqrt(phi2corr / min((kcorr-1), (rcorr-1))) #returns one single value that is the Cramer's V value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Survived  Pclass    Sex  Embarked\n",
      "Survived     1.000   0.337  0.543     0.164\n",
      "Pclass       0.337   1.000  0.130     0.258\n",
      "Sex          0.543   0.130  1.000     0.111\n",
      "Embarked     0.164   0.258  0.111     1.000\n"
     ]
    }
   ],
   "source": [
    "# INSPIRATION:\n",
    "# https://www.kaggle.com/chrisbss1/cramer-s-v-correlation-matrix\n",
    "# https://stackoverflow.com/questions/20892799/using-pandas-calculate-cram%C3%A9rs-coefficient-matrix\n",
    "\n",
    "\n",
    "# catgorical variables\n",
    "cat_vars = [\"Survived\", \"Pclass\",\"Sex\", \"Embarked\"] #create iterable conatining the column names we want to apply a Cramer's V computation with each other\n",
    "\n",
    "#create empty table to store the Cramer's V values\n",
    "# cramersV_table = pd.DataFrame() #-1 everywhere, nrows = len(cat_vars), ncols= len(cat_vars), column names=cat_vars)\n",
    "# cramersV_table = pd.DataFrame(-1, index=np.arange(len(cat_vars)), columns=cat_vars) \n",
    "\n",
    "#cramersV_table = pd.DataFrame(data = -1, nrows = len(cat_vars), ncols= len(cat_vars), columns = cat_vars)\n",
    "\n",
    "# index = cat_vars\n",
    "\n",
    "cramersV_table = pd.DataFrame(-1, index=cat_vars, columns=cat_vars)\n",
    "\n",
    "#IMPORTANT: don't forget pd.DataFrame.loc[row,col] in order to retrieve a specific row and/or column in the dataframe !!\n",
    "\n",
    "for col in cat_vars:\n",
    "    for row in cat_vars:\n",
    "        \n",
    "        if row == col:  # no need to compute, we know that comparing exactly same vars gives a dependence = 1 \n",
    "            cramersV_table.loc[row, col] = 1\n",
    "            cramersV_table.loc[col, row] = 1\n",
    "            \n",
    "        elif cramersV_table.loc[row, col] == -1 : #if the cramers V has not been computed yet, then we will compute it\n",
    "            \n",
    "            # COMPUTE CRAMER'S V\n",
    "            confusion_matrix = pd.crosstab(train_set[col], train_set[row])    # create the confusion matrix\n",
    "            cramer_result = round(cramers_v(confusion_matrix.values), 3)      # compute the Cramer's V\n",
    "            #print(\"var 1: \"+col+\" | var 2: \"+row+\" | Cramer's V: \"+str(cramer_result))\n",
    "            \n",
    "            # ADD CRAMER'S V to the results in the table\n",
    "            cramersV_table.loc[row, col] = cramer_result\n",
    "            cramersV_table.loc[col, row] = cramer_result\n",
    "        \n",
    "print(cramersV_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASsAAAD8CAYAAAAv6IKXAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAwuElEQVR4nO3dd3wUdfrA8c+TggQCBEISOEBBQJEmJwjqqWDHClJVDk4sqKeeej8roiiI4iHnWU6RQwVRERTxUDnsFCkiKFIE6V1CkBaQkmye3x8zCZu6E9mSXZ83r3kxszsz+0x28+Q7s9/5PqKqGGNMRRcX6QCMMcYLS1bGmKhgycoYExUsWRljooIlK2NMVLBkZYyJCpasjDFBJSKvicgOEVlWyvMiIs+LyBoRWSIip3nZryUrY0ywjQU6l/H8pUBTdxoAvOxlp5asjDFBpaqzgF1lrNIFeEMd84EUEakbaL8JwQqwNEl/vCNmu8h/PmlopEMIickrMiMdQkgMPL9JpEMImdrJCXIs25fn9/TQ4n/fgtMiyjdaVUeX4+XqAZv9lre4j/1c1kYhT1bGmCgg3k+y3MRUnuRU7NVK2m2gjSxZGWNAjqlhVl5bgAZ+y/WBbYE2smtWxhinZeV1OnZTgX7ut4JnAHtVtcxTQLCWlTEGgtqyEpEJQCegtohsAQYDiQCqOgqYBlwGrAF+Bfp72a8lK2MMxMUHbVeqem2A5xW4vbz7tWRljAnW6V1IWbIyxoT7AvtvYsnKGBP9LSsRyaaM/g+qWj3oERljwi/aW1aqWg1ARIYA24HxOB26+gDVQh6dMSY8or1l5ecSVe3gt/yyiHwD/CMEMRljwi2I3waGitd06hORPiISLyJxItIH8IUyMGNMGIW3U+hv4vWVrwN6AZnu1NN9zBgTC+LE+xQhnk4DVXUDzrAOxphYFAXXrDxFKCInicgX+SP/iUhrERkU2tCMMWEj4n2KEK/p9D/AQ0AOgKouAa4JVVDGmDCLi/c+RYjXbwOrqOoCKZxVc0MQjzEmEqLgNNBrstopIo1xO4iKSA8CjOpnjIki0d4p1M/tOCMDNhORrcB6nI6hxphYEEMtq42qeqGIVAXiVDU7lEEZY8IsClpWXtPpehEZDZwB7A9hPMaYSIihTqEnA5/jnA6uF5EXReTs0IVljAmrIH8bKCKdReQnt5DpgyU8X1NEprhFTheISMtA+/TaKfQgMAmYJCI1geeAmUCFuKFo1OA+XHpuS7J2ZdOu55ORDqdcli6ax9ujn0Xz8jjn4qu4vGe/Qs9/P38WU958BZE44uLjufbmuzmpRZuC5/N8Pobc05+U1DTuHjwyzNGXrVlaVbq2SidOhPkb9/DlmsKl5BqnVuGG9vXY9WsOAEt/zubTVb8UPC/APR0bsvdgLq8u2BLO0Ms0f+5s/vXMcPJ8Pq7s2p2+/W8u9PzG9esY9vggVq38kQF/vYvr+h0dtTc7ex/Dhz7KujVrEBEGDh5Ky9ZtwnwEJQhii0lE4oF/AxfhFIf4VkSmquqPfqsNBBar6tUi0sxd/4Ky9ut5PCsR6Qj0xqmm+i3O7TcVwvgP5zNq4kzGDO0XeOUKJM/n482Xn+H/nnieWqnpDLmnP206nEO94xsVrHPKqe1o0+EcRITN61fz8tODeHLUxILnP5s6kboNGnLw1wOROIRSCdCtdQaj5m1m78Ec7jm3Icu37ydz/5FC66375WCpiejcE2uyI/swxyVUiL+JAPh8PkYOH8a/XvoP6RkZ3NS3N2d3PI9GJx6tSVi9Rg3uue8hZs34stj2/xrxFB3OPJth//gXOTlHOHToUDjDL11wr1m1B9ao6jpn1/IOzh0w/smqOfAUgKquFJGGIpKhqqUWrfTag309cDcwG2ipqr1UdfJvOowQmPPdWnbt/TXSYZTbulU/kl63Pul16pGQmEiHcy9i8fxZhdapnFSF/P5thw8dKlRwbdfOHSz5di7nXnxVGKP25vialdl54Ai7fs3Bp/D91n20rJPsefsalRM4JSOZ+Zv2hjDK8luxfCn1GzSgXv0GJCZW4oKLL2P2jK8KrVOzViqntGhFQkLhtsCB/fv54ftFXNm1OwCJiZWoVq2CDAlXjmtWIjJARBb6TQOK7K20Iqb+fgC6AYhIe+AEnJJcpfLasjpVVfd5XNd4tOeXLGqlpRcs16ydzrqflhdbb9HcGUx+42Wy9+zmLr9TvQmjn6XnDXdwqIK1qgBqVE5kz8Gj/Yb3HMrlhJpJxdZrWCuJezs2ZO+hXKb+uIPMbKfl1bVlOh/9uKNCtaoAsnZkkp5xtNJ5ekYGy5ct8bTt1q2bSalZk2GPPcya1T9xcrMW3H3fgyQlVQlVuN6Vo2XlociplyKmw4HnRGQxsBT4ngAdzctsWYnI/e7sMBF5vuhU1rYmMC1hENaSPjNtz+rEk6Mmcsegp5ny5isALF7wNdVTatKwSbNQh/mblPhpLXK4W/YeYuhna3hm5ga+Xr+bG053/rA2z6jK/sM+tuw9HPpAy6noMQAFLd9AfD4fq1au4Ooe1zD27ckkJSUx/vUxQY7wNwrut4EBi5iq6j5V7a+qbYB+QBpO/81SBWpZrXD/X+glwnxus3AAQEL9TiTUblGezX83aqamsytrR8Hy7p07SKmVVur6J7f8I1nbt5K9dw9rflzC4m9ms2ThXHKOHOHQwQOMfmYwA+59PByhB7TnUA4pSUc/XimVE9h3KKfQOodz8wrmV+w4QPc4oWqleBrVqkKLOsmckpFMQpxQOSGOPqfV5a3vIn/TRHpGBjsyj8axIzOT2rXTy9jCb9v0DNLSM2jRqjUAnS68mDcrSLKSuKB2SfgWaCoijYCtOPcRFxpSSkRSgF9V9QhwEzAr0NlboGGNP3Rnl6jq914j9W8mJv3xjoA17H+vGp10CpnbNpO1fRs1U9P4ZtZn3HLfkELrZG7bTHrd+ogIG9esJDcnl+TqNehx/V/pcf1fAVi5ZBHTp7xdYRIVwOY9h0irWolaVRLZezCHP9arzvjvClcIr3ZcPNmHnTEcj0+pjAAHjvj4eEUWH6/IApxvDDs1rlUhEhVAs+Yt2bJ5E9u2biEtPZ0vPp3G4GEjPG2bWjuN9Iw6bNywnhMaNmLRgvk0PLFxiCP2xmvr0AtVzRWRO4BPcHoMvKaqy0XkVvf5UcApwBsi4sO58H5joP16vWb1TxGpC7wLvKOqxS+sRNC4p67nnLZNqZ2SzJrpQxk6ahrjPpgX6bACio9P4M+33ss/H72LvLw8zr7oCuqdcCJfTXsfgPMu68aiuV8x98v/ER+fQKVKx3HrA0OD+sEKlTyF95dmMuCMBsQJLNi0l8zsI5x5QgoA8zbu4dS61TirYU3yVMnxKeMXbSt7pxVAQkIC99z/MH+/YwA+Xx5XdLmaExs3Ycp7zje0V/fozS87s7ixb28OHNhPnMQxacJ43np3KlWTk7nn/oE8PugBcnNy+EO9+gx87IkIH5EryB8pVZ2GU3nZ/7FRfvPzgKbl2adoSSfhJa0oUgenu0JvoDowUVUD/qRjuWX1+aShkQ4hJCavKPXb46g28PwmgVeKUrWTE44p3ST3Guv593T/pOsj8tfS84mqqm5X1eeBW4HFwKOhCsoYE14i4nmKFE+ngSJyCk6LqgfwC/AO8H8hjMsYE0Zxwb3AHhJer1m9DkwALlbVin9hwRhTPhX/MmjgZOXe57NWVZ8LQzzGmAiIhi9tAiYrVfWJSKqIVHL7RBhjYkxMJCvXRmCOiEwFCu7tUNV/hiQqY0xYxVKy2uZOcUC10IVjjImEmElWqlpxukYbY4JOIlhp2SuvXRe+ovhd06jq+UGPyBgTdjHTsgLu9ZuvDHTH6gYaEzNiJlmp6qIiD80RkZkhiMcYEwkVP1d5Pg2s5bcYB7QD6oQkImNM2MVMywpYxNFrVrnABjwM6WCMiQ5Rn6xE5HRgs6o2cpf/gnO9agOFB383xkSxaLg3MFCErwBHAETkXJxqFOOAvZQ9BrMxJppIOSYvuwtcN7CGiHwoIj+IyHIR6V/SfvwFOg2MV9X8Ym+9gdFuVZvJ7kDvxpgYEMzTQI91A28HflTVK0UkDfhJRN4q65a+QC2reBHJT2gXAP6F0DzXHDTGVGxBHs+qoG6gm3zy6wb6U6CaODtMBnYRoDtUoIQzAZgpIjuBgzh1AxGRJjingsaYGFCelpV/QRjXaLfuQr6S6gZ2KLKbF4GpOLfxVQN6q2oeZQhUMGKYiHwB1AU+1aNjIMcBd5a1rTEmepTndpsg1Q28BGfE4fOBxsBnIjK7rAo3XoaImV/CY6sCbWeMiR5B7roQsG4g0B8Y7jaA1rhV35sBC0rbacX/vtIYE3JBvmZVUDdQRCrh1A2cWmSdTTjXwRGRDOBkYF1ZO7WL5MaYSNQNHAqMFZGlOKeND6jqzrL2a8nKGBOJuoHbgIvLs8+QJ6tYra0HcGGvRyIdQkhkzX8h0iGExOZdv0Y6hJCpnXxsv8pRf7uNMeb3IS5WBt8zxsQ2a1kZY6JCFOQqS1bGGGtZGWOiRBTkKktWxhi7wG6MiRKWrIwxUcFOA40xUcEusBtjooIlK2NMVIiCXGXJyhhjF9iNMVEiGk4DPQ2+JyKNReQ4d76TiPxNRFJCGpkxJmxEvE+R4nWk0MmAzy0U8SrQCHg7ZFEZY8IqyCOFhoTXZJWnqrnA1cC/VPUenCISxpgYEOyWlYcip/eJyGJ3WiYiPhGpVdY+vSarHBG5FvgL8JH7WKLHbY0xFVwwW1Z+RU4vBZoD14pIc/91VHWEqrZR1TbAQ8BMv4LKJfKarPoDZwLDVHW9iDQC3vS4rTGmgouLE8+TB16KnPq7FqdGaZk8fRvoln3+G4CI1ASqqepwL9saYyq+8lyKClKR0/x9VQE6A3cEel1PyUpEZgBXuesvBrJEZKaq/t3L9saYiq08F86DVOQ035XAnECngOD9NLCGWym1G/C6qrYFLvS4rTGmggvyBXYvRU7zXYOHU0DwnqwSRKQu0IujF9iNMTEiAkVOEZEaQEfgv1526rUH+xCcgoVfq+q3InIisNrjtkGxdNE83h79LJqXxzkXX8XlPfsVev77+bOY8uYriMQRFx/PtTffzUkt2hQ8n+fzMeSe/qSkpnH34JHhDP2YjBrch0vPbUnWrmza9Xwy0uGUy9yvZ/PM08Pw5eXRtVsP+t84oNDz69ev4/FHHmLlih/565130+/6GwE4fPgwN/f/M0eOHMHn83HBhRdz6+1/i8QhlOi7b+Yw5sVnyPP5uOjyq+nep3+h52d+No33J4wFoHJSFW69ZyCNmpwEwP7sbP49Ygib1q9FBO54YDDNWpwa7kMoJgJFTsHpCvWpqh7wsl+vF9jfBd71W14HdC9H/Mckz+fjzZef4f+eeJ5aqekMuac/bTqcQ73jGxWsc8qp7WjT4RxEhM3rV/Py04N4ctTEguc/mzqRug0acvBXTz+XCmP8h/MZNXEmY4b2C7xyBeLz+Rj+5BBeGv0aGRkZ9L22Jx07nc+JjZsUrFOjeg3ue3AQM778vNC2lSpVYtSYsVSpUpWcnBxu/Esf/nT2ubQ6tU2Yj6I4n8/HK889zePPvERqWgb33fpn2v+pIw0anliwTkbdegx7bgzJ1aqz6Js5vDTyCUa8/AYAr744gtPan8UDQ0aQk5PD4UOHInUohQT73sBARU7d5bHAWK/79Hq7TWURuV1EXhKR1/Inry9yrNat+pH0uvVJr1OPhMREOpx7EYvnzyq0TuWkKgV/HQ4fOlToCt+unTtY8u1czr34qnCFHDRzvlvLrr3RV5xz+bIlNDj+eOrXb0BiYiUu7nwZM776otA6tVJTadGyFQkJhf9mighVqlQFIDc3l9zc3AozLMDqlcuoW68+df5Qn8TERM4+/xK+mTOj0DrNWp5KcrXqAJzcvBW/ZGUC8OuB/Sz/4TsuvLwrAImJiSRXqxbO8EsVDbfbeD0NHA+sBC7BOSXsA6wIVVBF7fkli1pp6QXLNWuns+6n5cXWWzR3BpPfeJnsPbu5y+9Ub8LoZ+l5wx0cirJWVTTbkZlJRsbRmxwyMuqwbOkPnrf3+Xz8+ZrubN60iV7XXEer1pE/VQLYlZVF7bQ6Bcupaems/nFZqet//vEHnNb+TwBs37aVGik1eX74Y2xYu4rGJ53CTXfeR+WkpJDHHUjM3MgMNFHVR4ADqjoOuBxoVdrKIjJARBaKyML/vjP2mIPUEr71LOln2/asTjw5aiJ3DHqaKW++AsDiBV9TPaUmDZs0O+Y4jHclfU9dnl+I+Ph4Jrz7Af/7bAbLli1hzepVwQvuGJT0WSytubH0+2/5fNoH9LvFud6W5/OxdtVKLu3Sg2fHTKByUhKT3349lOF6Fkstqxz3/z0i0hLYDjQsbWX/fhhzVu8urX+FZzVT09mVtaNgeffOHaTUSit1/ZNb/pGs7VvJ3ruHNT8uYfE3s1mycC45R45w6OABRj8zmAH3Pn6sYZkyZGRkkJn5c8FyZuZ2avu1jr2qVr067dq1Z+6c2TRpelIwQ/xNUtPS2Zm1vWD5l6wd1Kpd/LO4Ye0qXhwxlEeffoHqNVIKtk1NS+ek5s7f+TM7XsD7b48NR9gBxcVQy2q023P9EZyvIH8E/hGyqIpodNIpZG7bTNb2beTm5PDNrM9o0+GcQutkbtuMqpMXN65ZSW5OLsnVa9Dj+r8yctyHjHjtA269fyjNWrezRBUGzVu0YvPGjWzdsoWcnCN8On0aHTud72nb3bt2kb1vHwCHDh3im/nzaNjoxABbhUfTk1vw85bNZP68lZycHL7+8hPan9Wx0DpZmT8z/JF7uWfgUOo1OKHg8ZqptamdnsHWTRsAWLJoAQ1OaERFEOTbbULC67eBY9zZmUDYPzXx8Qn8+dZ7+eejd5GXl8fZF11BvRNO5Ktp7wNw3mXdWDT3K+Z++T/i4xOoVOk4bn1gaFSchwcy7qnrOadtU2qnJLNm+lCGjprGuA/mRTqsgBISErh/4CPccduN+Hx5dOnancZNmvLepHcA6NHrGnbuzKLvNT04cGA/EhfHhDff4N0PPmbnziwGD3oQn8+H5ikXXtKZczueF+EjcsQnJHDzXQ/w+H2348vL48JLr+L4Ro2Z/t/3AOjcpQcTx/2H7H17GfXsU8428fGMHP0WADf/7QH++cTD5ObmkFG3Pn978LFIHUohUTBQKJLfGinxSZEyb6dR1X8GeoFgnAZWVBf2eiTSIYRE1vwXIh1CSGzeFX3fqnp1St2qx5RuLhu1wPPv6bRb20cktQVqWVWM71WNMSEVDSchZSYrVbWLO8b8DkiJ9x5XLF47hY7zH3NdRGqGs1OoMSa04sT7FCleuy60VtU9+QuqultE/hiakIwx4RZLpbjiRKSmqu4GcMdKtjJexsSIaOhn5TXhjATmici7OJ2TewHDQhaVMSasoiBXee5n9YaILATOxxkFsJs71LExJgZEQ5/EMpOViFQGbgWaAEuBUW5JLmNMDImCXBXw28BxQDucRHUp8EzIIzLGhF28iOfJi0B1A911Orl1A5eLyMxA+wx0GthcVVu5O34VWOApUmNMVAnmaaBf3cCLcMZj/1ZEpvpfOnK7Qr0EdFbVTSIS8C73QC2r/NEWsNM/Y2JXkPtZeakbeB3wvqpuAlDVHQQQKFmdKiL73CkbaJ0/LyL7PIVtjKnwylMwwn+8OncaUGR3JdUNrFdknZOAmiIyQ0QWiUjAcbsD3W4T7+VAjTHRrTxngUGqG5gAtAUuAJJwukbNV9VSR1m0jp3GmGB3XfBSN3ALsNOtbHNARGYBpwKlJiuvg+8ZY2JYfJx4njzwUjfwv8A5IpLglpDvQIC6DtayMsYEdcwFL3UDVXWFiEwHlgB5wBhVLb3yBpasjDEE/95Aj3UDRwAjvO7TkpUxJip6sFuyMsZE/72BxpjfhyjIVZasjDF4/ZYvoixZGWPsNBBg8orMUL9ExMRqyaq0M+6MdAghsfvbFyMdQoUVDR0urWVljLGWlTEmOkTBJStLVsYYu8BujIkSUZCrLFkZY6yflTEmSsRS3UBjTAyzrgvGmKgQBQ0rS1bGmOj4NjAaWn/GmBALcnWbgHUD3ZqBe926gYtF5NFA+7SWlTEmqBfYvdQNdM1W1Ss8xxi0CI0xUUvE++SBl7qB5WbJyhgT7NNAL3UDAc4UkR9E5H8i0iLQTu000BiDlKNkhFvU1L+w6Wi3luDR3RVXtG7gd8AJqrpfRC4DPgCalvW6lqyMMSSU4xzLQ5HTgHUDVXWf3/w0EXlJRGqr6s7SdmqngcaYcpWP9yBg3UARqSPuzkSkPU4u+qWsnVrLyhgT1BuZvdQNBHoAt4lILnAQuEZVi54qFmLJyhgT9B7sgeoGquqLQLmGbrVkZYyxG5mNMdEhPgquXnsKUURuLLIcLyKDQxOSMSbc4hDPU+Ri9OYCEZkmInVFpCUwH6gWwriMMWEU5B7sIeEpWanqdcA4YCnORbO7VfXeUAZWVLO0qjx4fiMGXnAi5zepVez5xqlVGHZpU/6vY0P+r2NDLj4ptdDzAvy9Y0NubF8/TBF7M/fr2XS7sjNdLr+Y118t3nVl/fp1XP/n3pzRthVvjH214PHDhw/T77qeXNOjCz2vvoJR/34+nGEfs1GD+7Dxi6dY+O7ASIdSbnNmz+Kqyy/his4X8ep/SnjP1q2l73W9ademJeNef7XQc48OeohO55xJty6eb4kLi2DfyBySGL2sJCJNgbuAycAGoK+IVAlhXIVfH+jWOoPR87fw9JfrOK1edTKSKxVbb90vBxk5cwMjZ27g01WFu2yce2JNdmQfDlPE3vh8PoY/OYTnX/4P733wEZ/872PWrV1TaJ0a1Wtw34OD6PuXGwo9XqlSJUaNGcs77/2XtydNYe6cr1n6w+IwRn9sxn84ny63/zvSYZSbz+fjyWFDeGnUGKZM/Zjp0z5i7ZrC71n1Gik88NDD/KX/jcW279K1Gy+/MiZc4XoWJ+J5iliMHtf7EHhUVW8BOgKrcTp+hcXxNSuz88ARdv2ag0/h+637aFkn2fP2NSoncEpGMvM37Q1hlOW3fNkSGhx/PPXrNyAxsRIXd76MGV99UWidWqmptGjZioSEwt+FiAhVqlQFIDc3l9zc3OgYQc0157u17Nr7a6TDKLdlS5fQoMEJ1G/QgMRKleh82eXF3rPU1FRatmpd7D0DaNvudKrXqBGucD2LmdNAoL2qfg6gjpFA15BFVUSNyonsOZhbsLznUC41khKLrdewVhL3dmzIzR3qk1HtaMura8t0PvpxB2V3OQu/HZmZZGTULVjOyKhD1g7vFax9Ph/X9uzKRZ3+xBlnnkWr1qeGIkzjZ0dmJnXq1ilYTs/IIDMz+quOx8eJ5ylSvCarJBF5VUSmA4hIc+Dc0lYWkQEislBEFi75ZNIxB1niXZFFEs+WvYcY+tkanpm5ga/X7+aG051rU80zqrL/sI8teyvWKSAUv7MTylcZNz4+ngnvfsD/PpvBsmVLWLN6VfCCMyXSEt61aKhmHEhcOaZI8fraY3G6zuc3A1YBd5e2sqqOVtV2qtqu9SW9jilAgD2HckhJOtqkTqmcwL5DOYXWOZybxxGf80FaseMA8XFC1UrxNKpVhRZ1khl0YWP6tv0DTWtXoc9pdakIMjIyyMz8uWA5M3M7tdPSy72fatWr065de+bOmR3M8EwJMjLqsP3n7QXLOzIzSU8v/3tW0QT53sCQ8JqsaqvqJCAPnHt/AF/Ioipi855DpFWtRK0qicQL/LFedZZl7i+0TrXj4gvmj0+pjAAHjvj4eEUWQz5byxOfr2X8om2s3vkrb333MxVB8xat2LxxI1u3bCEn5wifTp9Gx07ne9p2965dZO9zblw/dOgQ38yfR8NGJ4YyXAO0aNmKTZs2sGXLZnKOHGH6tI/peJ6396wik3JMkeK1B/sBEUnFPXMRkTOAsF2tzlN4f2kmA85oQJzAgk17ycw+wpknpAAwb+MeTq1bjbMa1iRPlRyfMn7RtrJ3WgEkJCRw/8BHuOO2G/H58ujStTuNmzTlvUnvANCj1zXs3JlF32t6cODAfiQujglvvsG7H3zMzp1ZDB70ID6fD81TLrykM+d2PC/CR+TduKeu55y2Tamdksya6UMZOmoa4z6YF+mwAkpISOChhx/ltgE3kZfno+vV3WnSpCmTJk4AoFfva9mZlcW1vbtzYP9+4uLieHP8OKZMnUZycjIP3Pt3Fn67gD17dnPR+edy2+130q17zwgfVXTcbiMBbnR2VhI5DXgBaAksA9KAHqq6JNC2f5+6soJd1g6eIZecHOkQQiLtjDsjHUJI7P62XPfNRpXKCcfW6Hlr0RbPv6d92taPSGYr8zRQRE4XkTqq+h1Ol4WBwGHgU5wBtowxMSAuTjxPEYsxwPOvAEfc+bOAh3GqVuym7JECjTFRJBq+DQx0zSpeVXe5871xxlqeDEwWkcUhjcwYEzbR0P0iUKKMF5H8hHYB8KXfcza8jDExItjfBgYqcuq33uki4hORHoH2GSjhTABmishOnKFHZ7sv0IQwfhtojAmtYLasvBY5ddd7GqcPZ0BlJitVHSYiX+B0Bv3Ub4zkOCA2vzIy5ncoPringQVFTgFEJL/IadGKzHfiDI5wupedBjyVU9X5JTxm93UYE0PKk6o81A0sqchphyL7qAdcDZxPsJKVMSb2ladh5aFuoJcip/8CHlBVn9dTUEtWxphgD1ccsMgp0A54x01UtYHLRCRXVT8obaeWrIwxwR6nqqDIKbAVp8jpdf4rqGqjo68tY4GPykpUYMnKGANIEFtWHouclpslK2NMsL8NDFjktMjj13vZpyUrY0xUjIhtycoYY8nKGBMdgnnNKlQsWRljIloP0CtLVsaYqBgp1JKVMcZOA40x0cFOA40xUcFaVsaYqBAFl6wsWRljIlsP0CtPpbiOxc79uTFbiisru+KVpA+GRmlVIx1CSNQ8/Y5IhxAyB79/8Zjyzfw1ezz/np7RJCUiuc1aVsaYqGhaWbIyxtgFdmNMdLAL7MaYqBAFuSqiBVaNMRVFkAsHBqobKCJdRGSJiCwWkYUicnagfZbZshKRbmU9r6rvBw7bGFPRBfPeQI91A78ApqqqikhrYBLQrKz9BjoNvNL9Px04i6MVmc8DZgCWrIyJAUE+DQxYN1BV9/utX5Xi1W+KCVTktL/7Yh8BzVX1Z3e5Lk7mNMbEguBmq4B1AwFE5GrgKZzG0OWBdur1mlXD/ETlygRO8ritMaaCk/L8ExngXmfKnwYU211xxVpOqjpFVZsBXYGhgWL0+m3gDBH5BJjgvug1wFcetzXGVHBBLnLqpW6g//5miUhjEamtqjtLW89TslLVO9wm27nuQ6NVdYqXbY0xFV+46waKSBNgrXuB/TSgEvBLWTstTz+r74BsVf1cRKqISDVVzS7XIRhjKqQI1A3sDvQTkRzgINBbA9yo7ClZicjNwACgFtAY5wLaKOCC33g8xpgKJNg92APVDVTVp4Gny7NPrxfYbwf+BOxzX2g1zhV8Y0wMCHKf0JDwehp4WFWPiJt+RSQBD/0ijDFRIgrut/HaspopIgOBJBG5CHgX+DB0YRljwqk8XRcixWuyehDIApYCtwDTVPXhkEVljAmrOPE+RYrX08DHVPVR4D/g3PsjIm+pap/QhWaMCZsYOg08XkQeAhCRSjj3BK4OWVTGmLCKpdPA/kArN2F9BMxQ1cdCFpUxJqxEvE+REmiImNP8Fp8DXgHm4FxwP01VvwtlcMaY8IiCs8CA16xGFlneDTR3H1fg/FAEZYwJsyjIVmWeBqrqeTi91Eep6nlFprAmqvlzZ3NNt8vp1aUz41//T7HnN65fx4Drr6PTGW14+43XCz2Xnb2Ph++/m2u7XcF13a9k2ZLFYYo6sO++mcNf+17NrdddxeS3Xi/2/MzPpnHXDb2464ZePHD79axfs6rguf3Z2Tz96H3c3rcbd/TrxsrlP4Qz9IDmzJ7FVZdfwhWdL+LV/xS/73X9urX0va437dq0ZNzrrxZ67tFBD9HpnDPp1uWKcIUbFKMG92HjF0+x8N2BkQ6lXOJEPE8RizHQCqqah9ODPWJ8Ph8jhw9j5POjeOu9qXz+yTTWr1tTaJ3qNWpwz30PcW3f/sW2/9eIp+hw5tlMeP8jxr0zmRManRiu0Mvk8/l45bmnefTpF3hh3GRmfzmdzRvWFVono249hj03hudem0Svfjfz0sgnCp579cURnNb+LP49/n2efXUi9Y+vGMcFzrE9OWwIL40aw5SpHzN92kesXVP0PUvhgYce5i/9byy2fZeu3Xj5lTHhCjdoxn84ny63R99Qb9HQg93rBfbPROReEWkgIrXyp5BG5mfF8qXUb9CAevUbkJhYiQsuvozZMwqPUFOzViqntGhFQkLhM9sD+/fzw/eLuLJrdwASEytRrVr1cIVeptUrl1G3Xn3q/KE+iYmJnH3+JXwzZ0ahdZq1PJVkN96Tm7fil6xMAH49sJ/lP3zHhZd3BSAxMZHkatXCGX6Zli1dQoMGJ1C/QQMSK1Wi82WXM+OrLwqtk5qaSstWrYu9ZwBt251O9Ro1whVu0Mz5bi279v4a6TDKLwqyldd+Vje4//u3sBQIy5/yrB2ZpGfULVhOz8hg+bIlnrbdunUzKTVrMuyxh1mz+idObtaCu+97kKSkKqEK17NdWVnUTqtTsJyals7qH5eVuv7nH3/Aae3/BMD2bVupkVKT54c/xoa1q2h80incdOd9VE5KCnncXuzIzKRO3aPHlp6RwdIl3t4zE37RUDfQU8tKVRuVMIXtnKOkgSPE47mzz+dj1coVXN3jGsa+PZmkpCTGv14xTi+0pNsrSzmupd9/y+fTPqDfLX8DIM/nY+2qlVzapQfPjplA5aQkJr9d/JpXpJR0bF7fMxN+0dB1wXMpLhFpKSK9RKRf/lTGugXDnr7xWvGL4eWVnpHBjsyjoyrvyMykdm1vgz6kp2eQlp5Bi1atAeh04cWsWrnimGMKhtS0dHZmbS9Y/iVrB7VqpxVbb8PaVbw4YigPDXuW6jVSCrZNTUvnpOatADiz4wWsW70yLHF7kZFRh+0/Hz22HZmZpKfbQB0VVcwkKxEZDLzgTucB/wCuKm19VR2tqu1UtV2/G24+5iCbNW/Jls2b2LZ1Czk5R/ji02mc3fE8T9um1k4jPaMOGzesB2DRgvk0PLHxMccUDE1PbsHPWzaT+fNWcnJy+PrLT2h/VsdC62Rl/szwR+7lnoFDqdfghILHa6bWpnZ6Bls3bQBgyaIFNDihUTjDL1OLlq3YtGkDW7ZsJufIEaZP+5iO51lPl4oqGnqwS4DB+ZyVRJYCpwLfq+qpIpIBjFHVKwNsys79uUEZSmbu17N4fuRwfL48ruhyNX+58RamvDcRgKt79OaXnVnc2Lc3Bw7sJ07iSKpShbfenUrV5GRW/bSC4UMHk5uTwx/q1WfgY09QvfqxX7zNyj58zPtYOP9rXnvxGXx5eVx46VX07HsT0//7HgCdu/TgxX8MYd6sL0hzr9nFx8czcvRbAKxb/RP/HjGE3NwcMurW528PPlZwMf5YNEqresz7AJg9ayb/GP4keXk+ul7dnZtvuY1JEycA0Kv3tezMyuLa3t05sH8/cXHOezZl6jSSk5N54N6/s/DbBezZs5taqancdvuddOve85jiqXn6HcE4rDKNe+p6zmnblNopyezYtY+ho6Yx7oN5IX/dg9+/eExZZNOuw55/T4+vdVzA1xKRzjgdyeNxcsXwIs/3AR5wF/cDt6lqmX1vvCarBaraXkQW4bSssoFlqtoi0LbBSlYVUTCSVUUUrGRV0YQjWUXKsSarzeVIVg0CJCu3yOkq/IqcAtf6FzkVkbOAFaq6W0QuxRksoVi5Ln9evw1cKCIpOKMuLMLJhAs8bmuMqeCCfC3KS5HTuX7rz8epgFMmr9Vt/urOjhKR6UB1VbXvoY2JGd6zlVsn0L9W4Gi3PFc+T0VO/dwI/C/Q63qubiMi3YCzcfpXfQ1YsjImRpRnUD0PdQM9FTkFEJHzcJLV2YFe12t1m5eAJjhFTgFuEZELVTWit+EYY4IjyKeBnoqcikhrYAxwqaqWWTMQvLesOgIt8+t6icg4nCGOjTExIMhdErwUOT0eZxDPvqq6qvguivOarH4Cjgc2ussNsNNAY2JHEHOVxyKnjwKpwEvunQ25qtqurP0GGnzvQ5xzzRrAChFZ4C53AOaWta0xJnoEu6unhyKnNwE3lWefgVpWz5RnZ8aY6BQNt22WmaxUdab/sohUD7SNMSb6RMNN5l6/DRwADAUOAnk4rcawDRFjjAmtip+qvLeS7gNaqOrOUAZjjImMKGhYeU5Wa4EoHP7QGONFNAy+5zVZPQTMFZFvgIK7d1X1byGJyhgTVrHUsnoF+BKnI2he6MIxxkRCLCWrXFX9e0gjMcZETCydBn7lfiP4IYVPA3eFJCpjTFjFUssq/76eh/wes64LxsSIKMhVnsezqjiDextjgi8KslWZBSNE5H6/+Z5FnnsyVEEZY8IrGgpGBKpuc43f/ENFnusc5FiMMRESJ96nSAl0GiilzJe0bIyJVlHw2xwoWWkp8yUtG2OiVCx0XThVRPbh5N0kdx53uXJIIzPGhE00dF3wVDcwWojIgCJVNmJGrB6bHZfxylP5+CgyIPAqUStWj82Oy3gSa8nKGBOjLFkZY6JCrCWrWL5GEKvHZsdlPImpC+zGmNgVay0rY0yMsmRljIkKEU1WIvKwiCwXkSUislhEOgRhn1eJyINBim9/MPbjtz+fe5zLRORdEalSxrqPici9wXz9SAnF+xyEmPLfi/zJ82dGRDqJyEfH+PozRKTMCsRlbDtWRHocy+tHo4jVABSRM4ErgNNU9bCI1AYqedw2QVVzS3pOVacCU4MXaVAdVNU2ACLyFnAr8M+IRhRix/I+h1jBexFuIhIfideNdpFsWdUFdqrqYQBV3amq20Rkg/uBRkTaicgMd/4xERktIp8Cb4jINyLSIn9n7l+qtiJyvYi8KCI13H3Fuc9XEZHNIpIoIo1FZLqILBKR2SLSzF2nkYjME5FvRWRoiI9/NtDEfd1+bqvjBxEZX3RFEbnZjekHEZmc3yITkZ5uK+0HEZnlPtZCRBa4rYUlItI0xMcRSGnvc1sRmem+B5+ISF33PftJRE4GEJEJInJzOIN1PzNPup+DhSJymhvfWhG51W/V6iIyRUR+FJFRfp+zl93tlovI40X2+6iIfA309Hs8TkTGicgTIhIvIiPc93qJiNziriPuZ/pHEfkYSA/Tj6NiUdWITEAysBhYBbwEdHQf3wDUdufbATPc+ceARUCSu3wP8Lg7XxdY5c5fD7zozv8XOM+d7w2Mcee/AJq68x2AL935qUA/d/52YH+Qj3m/+3+CG9ttQAvgJ79jruV3vPe686l++3gCuNOdXwrUc+dT3P9fAPq485Xyf14V6X0GEoG5QJrfe/OaO38RMA9neKLpIYzL58aVP/X2+/zd5s4/CywBqgFpwA738U7AIZyRcuOBz4AeRd6/eGAG0Npvv/f7vf4M4AxgAvCw+9gAYJA7fxywEGgEdHNfIx74A7An//V+T1PETgNVdb+ItAXOAc4DJnq4bjBVVQ+685Nw3sDBQC/g3RLWn4jzi/AVzof/JRFJBs4C3pWjd28e5/7/J6C7Oz8eeLq8xxVAkogsdudnA68CtwDvqVtAVkse176liDwBpOD88n/iPj4HGCsik4D33cfmAQ+LSH3gfVVdHeRjKJeS3mechNsS+Mx9D+KBn931PxNnoMd/A6eGMLSyTgPzLyMsBZJVNRvIFpFDIpLiPrdAVdeB0wIEzgbeA3qJU68gAeePaHOchAfOsft7BZikqsPc5YuB1n7Xo2oATYFzgQmq6gO2iciXv+WAo13EkhWA+8OfAcwQkaXAX4Bcjp6eFh3Z4YDftltF5BcRaY2TkG4p4SWmAk+JSC2gLU45sarAnjI+qKHseFbsF0Sc39ZArzkW6KqqP4jI9Th/2VHVW8W5WH05sFhE2qjq2+LUd7wc+EREblLViH64S3ifbweWq+qZRdd1T6dOAQ4CtYAtYQw1X35RlDy/+fzl/N+ZYkMmiUgj4F7gdFXdLSJjKfwZPlBkm7nAeSIyUlUP4YxmcqeqfuK/kohcVsLr/e5E7JqViJxc5HpKG2AjTnO5rftYd8r2DnA/UENVlxZ9UlX3AwuA54CPVNWnqvuA9e5f7/zrAfl/wedwdHTUPuU+qN/mC5y/xqluPLVKWKca8LOIJPrHJSKNVfUbVX0U2Ak0EJETgXWq+jxOsm4d8iMoQynv8wogTZyL74hzHTH/+uM97vPXAq+5x1wRtXevccbh/LH8GqiOk5D2ikgGcGmAfbwKTMNp5SfgtJhvyz9mETlJRKoCs4Br3GtadXFaqL87kWxZJQMvuM3qXGANzjn7KcCrIjIQ+CbAPt7DSURlXQyfiHOK2MnvsT7AyyIyCOf6yTvAD8BdwNsichcwuZzH85uo6nIRGQbMFBEf8D3OdTd/j+D8LDbinJpUcx8f4SYCwUl6PwAPAn8WkRxgOzAk5AdRttLe59HA8yJSA+dz+C835puA9qqaLc6XBoNwTvWDzf+UHJzrY+Xp8jIPGA60wkkmU1Q1T0S+B5YD63D++JVJVf/p/gzG43wuGwLfuS3uLKArMAU4H+e9XwXMLEecMcNutzHGRAXrwW6MiQqWrIwxUcGSlTEmKliyMsZEBUtWxpioYMnKGBMVLFkZY6LC/wPDDxt+2H6GmgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PLOTTING THE DATA\n",
    "\n",
    "# in R: using the corrplot library\n",
    "# #library(corrplot)\n",
    "# Create our own palette of colors (blue)\n",
    "#own_col = colorRampPalette(c(\"aliceblue\",\"lightblue\",\"lightskyblue\",\"lightslateblue\",\"blue1\",\"blue3\",\"darkblue\"))\n",
    "#corrplot(dependencies_matrix_manual, type=\"upper\", method = \"circle\", col = own_col(6), is.corr=FALSE, cl.lim=c(0,1), tl.col = \"black\") #order=\"hclust\"\n",
    "\n",
    "#sns.color_palette(\"Blues\", as_cmap=True)\n",
    "# in Python: using Seaborn\n",
    "# seaborn.heatmap(data, *, vmin=None, vmax=None, cmap=None, center=None, robust=False, \n",
    "#                 annot=None, fmt='.2g', annot_kws=None, linewidths=0, linecolor='white', \n",
    "#                 cbar=True, cbar_kws=None,cbar_ax=None, square=False, xticklabels='auto', \n",
    "#                 yticklabels='auto', mask=None, ax=None, **kwargs)\n",
    "\n",
    "# color palettes can be found at: https://seaborn.pydata.org/tutorial/color_palettes.html?highlight=color%20palette%20options\n",
    "# Interesting palettes names: Blues, crest, YlOrBr, seagreen, light:b, rocket\n",
    "\n",
    "sns.heatmap(data = cramersV_table,\n",
    "            cmap = sns.color_palette(\"Blues\", as_cmap=True),\n",
    "            square = True,\n",
    "            annot = True\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this process is to select a minimum amount of data (minimize redundancy) while keeping as much information as possible (maximize relevancy). Therefore, we need to:  \n",
    "- 1. identify the variables that are strongly dependent of each other\n",
    "- 2. amongst those, select the ones to keep (and the others to remove), based on:\n",
    "    - the dependency with the dependent variable (Y): the higher the better\n",
    "    - the number of classes they have: if they have roughly the same dependency with the dependent variable (Y), it is better to keep the independent variables (X) that contain less classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "\n",
    "- There does not seem to be much redundancy in the information carried by the different X explicative variables.  \n",
    "- The Y variable does not seem to be very well explained by ```Embarked``` (16%)  \n",
    "- The most important variable to predict Y seems to be ```Sex```, as it has the strongest dependence.\n",
    "- Since ```Sex``` and ```Embarked``` are not very strongly dependent on each other, it is possible that ```Sex``` adds additional information that is not already contained in ```Embarked``` to explain the Y variable (```Survived```). Hence, removing ```Sex``` might not be a good idea as we cannot show that it is very redundant with ```Embarked```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical variables: adapted mRMR (NOT DONE YET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we propose a feature selection based on mRMR (function called ```adapted_mRMR```). While the mRMR is usually used to compare continuous variables with each other, based on their correlation, here we try to adapt the technique to categorical variables, by using the Cramer's V as statistic that measures the correlation/dependency (which is itself based on a Chi-Squared test)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Association between continuous variables: Pearson correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe the dependencies between the continuous variables. For that purpose, we decide to use the well-known Pearson correlation. For two continuous variables $X_A$ and $X_B$, the Pearson coefficient is computed by: \n",
    "\n",
    "$$\\rho(X_A,X_B) = \\frac{Cov(X_A,X_B)}{\\sqrt{Var(X_A)}\\sqrt{Var(X_B)}}$$\n",
    "\n",
    "This correlation captures the linear dependence between the two quantitative variables. This correlation will always be between -1 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Age', 'Fare', 'SibSp', 'Parch']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.096688</td>\n",
       "      <td>-0.233296</td>\n",
       "      <td>-0.172482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fare</th>\n",
       "      <td>0.096688</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.159651</td>\n",
       "      <td>0.216225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SibSp</th>\n",
       "      <td>-0.233296</td>\n",
       "      <td>0.159651</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.414838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Parch</th>\n",
       "      <td>-0.172482</td>\n",
       "      <td>0.216225</td>\n",
       "      <td>0.414838</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Age      Fare     SibSp     Parch\n",
       "Age    1.000000  0.096688 -0.233296 -0.172482\n",
       "Fare   0.096688  1.000000  0.159651  0.216225\n",
       "SibSp -0.233296  0.159651  1.000000  0.414838\n",
       "Parch -0.172482  0.216225  0.414838  1.000000"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(train_set.columns)\n",
    "# print(cat_vars)\n",
    "# check the remaining_vars = [item for item in train_set.columns if item not in cat_vars]\n",
    "\n",
    "continuous_vars = [\"Age\", \"Fare\", \"SibSp\", \"Parch\"] #PassengerId is \n",
    "print(continuous_vars)\n",
    "\n",
    "# STRAIGHTFORWARD CORRELATION MATRIX IN PANDAS:\n",
    "train_set[continuous_vars].corr(method='pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Age   Fare  SibSp  Parch\n",
      "Age    1.000  0.097 -0.233 -0.172\n",
      "Fare   0.097  1.000  0.160  0.216\n",
      "SibSp -0.233  0.160  1.000  0.415\n",
      "Parch -0.172  0.216  0.415  1.000\n"
     ]
    }
   ],
   "source": [
    "# Pearson Correlation matrix, made \"by hand\"\n",
    "\n",
    "corr_matrix = pd.DataFrame(-1, index=continuous_vars, columns=continuous_vars)\n",
    "\n",
    "for col in continuous_vars:\n",
    "    for row in continuous_vars:\n",
    "        \n",
    "        if row == col: # no need to compute, we know that comparing exactly same vars gives a dependence = 1\n",
    "            corr_matrix.loc[row, col] = 1\n",
    "            corr_matrix.loc[col, row] = 1\n",
    "        \n",
    "        elif corr_matrix.loc[row, col] == -1: #if the corr has not been computed yet\n",
    "            \n",
    "            #compute Pearson correlation, and extract the result (at index 0)\n",
    "            pearson_corr = ss.pearsonr(train_set[row], train_set[col])[0]        # using SciPy\n",
    "            #pearson_corr = np.corrcoef(train_set[row], train_set[col])[0,1]     # using numpy\n",
    "            #print(pearson_corr)\n",
    "            \n",
    "            # ADD Pearson correlation to the results in the table\n",
    "            corr_matrix.loc[row, col] = round(pearson_corr,3)\n",
    "            corr_matrix.loc[col, row] = round(pearson_corr,3)\n",
    "\n",
    "print(corr_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PLOTTING THE DATA\n",
    "\n",
    "# color palettes can be found at: https://seaborn.pydata.org/tutorial/color_palettes.html?highlight=color%20palette%20options\n",
    "# Interesting palettes names: Blues, crest, YlOrBr, seagreen, light:b, rocket\n",
    "\n",
    "sns.heatmap(data = corr_matrix,\n",
    "            cmap = sns.color_palette(\"Blues\", as_cmap=True),\n",
    "            square = True,\n",
    "            annot = True\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "\n",
    "- There does not seem to be much redundancy contained in several X variables. A high correlation (e.g. >80%) would indicate that two variables contain pretty much the same information. That's not the case here.\n",
    "- In other words: assuming that these variables all help predict the Y variable (categorical), they all contain 'new'/different information (i.e. very limited redundant information).\n",
    "- Nothing can be said about the relevancy to predict Y since the Y variable is categorical and is not assessed here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection: Summary\n",
    "\n",
    "- The ```Sex``` (categorical) variable is very interesing as it is dependent with Y (54%) and is not redundant with the other X variables --> Max relevancy, minimum redundancy.\n",
    "- All continuous variables seem interesting: ```Parch```, ```SibSp```, ```Fare```, ```Age```. Eventually ```Parch```and ```SibSp```might be redundant (41%)\n",
    "- All categorical variables seem interesting : ```PClass```, ```Sex```, ```Embarked```.\n",
    "\n",
    "Conclusion: nothing should be removed at this stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection\n",
    "\n",
    "\n",
    "We are facing a classification problem where we want to predict wether someone survived or not, based on some input variables.\n",
    "\n",
    "## Procedure used to validate a model & its hyperparameters (REREAD)\n",
    "\n",
    "The step used to test a certain model & choose its hyperparameters are the following:\n",
    "\n",
    "1. Supply several possible combination of hyperparameters\n",
    "2. For each combination, test the model by cross-validation using 5 folds. The metric used to evaluate the performance is the average F1-score of each class (which is then averaged across folds)\n",
    "3. After all the combinations have been tested, select the one which led to the highest total accuracy because it is the target measure of the challenge. We also decide to compute the f1-score because it tends to penalize models which never predict one of the variables, which is often the case in unbalanced datasets. We therefore believe it is a more representative measure of the learning of the algorithm which might give valuable insights in terms of functionning of the model. It might also be used to compare models with more or less the same accuracy. \n",
    "4. Compute the performance (the average cross-validated F1-score) of the model & if possible try to interpret its result\n",
    "5. Compare the performance to the other tested models\n",
    "\n",
    "\n",
    "For models which use distance as part of their learning process (e.g. knn, radial basis function), we use some pre-processing on the data before feeding it to the algorithm.\n",
    "\n",
    "1. Transform categorical variables into binary ones because the algorithm cannot use them in their distance computation.\n",
    "2. Scale the variables to prepare them to PCA because PCA tries to maximize the variance of the new dimension created, therefore, it will predominately use variables with higher scales to compute those\n",
    "3. Uncorrelate variables using PCA. The goal is to avoid having correlated variables in distance computations because if two variables are heavily correlated, it probably means they hold more or less the same information. Therefore, distance computed on those dimensions would be double-counted. \n",
    "4. Rescale again the newly created dimensions for the same reason as in 2 (indeed there is no reason that the new dimensions have the same scale). \n",
    "\n",
    "\n",
    "\n",
    "The F1-score is defined by class via the formula: $2 *\\frac{precision * recall}{precision + recall}$ = Harmonic mean between precision & recall, it ensures to have the right balance between often predicting a class when it is actually the true class & when we have an observation of that class, we have a high probability to actually predict it right. However, the accuracy is the score used in the competition, so we decided to use it as the main criteria for comparing two models, even though it is not the most relevant one when we have unbalanced data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised learning\n",
    "\n",
    "The process of supervised learning involves the presence of an entity (the learner, also called prediction model), whose goal is to learn the mapping between inputs and outputs in a given problem.\n",
    "\n",
    "A supervised learning problem can formulated as follows:\n",
    "\n",
    "\\begin{equation*}\n",
    " y = m(\\mathbf{x})  \n",
    "\\end{equation*}\n",
    "\n",
    "where:\n",
    "- $y$ represents the output variable (also called target)\n",
    "- $\\mathbf{x}$ represents the vector of inputs (also called features).\n",
    "- $m$ is the (unknown) mapping between input and outputs.\n",
    "\n",
    "In the majority of the supervised learning problems, the mapping $m$ between input and outputs is unknown and needs to be estimated on basis of the available input/output observation pairs $(\\mathbf{x}_i,y_i)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "Both classification and regression are sub-fields of *supervised learning*. In the two cases, we have predictive variables $\\mathbf{x}$ and a target variable $y$. \n",
    "The main difference betweet the two type of problems is the type of the target variabile:\n",
    "\n",
    "- In classification, $y$ is a discrete variable; i.e $y \\in \\{C_1,\\cdots,C_k\\}$\n",
    "- In regression, $y$ is a continuous variable; i.e $y \\in \\mathbb{R}$\n",
    "\n",
    "In this particular case we are dealing with the simplest classification case, with $k=2$ possible output classes (called binary classification).\n",
    "\n",
    "The goal of our approach is to be able to learn the mapping between the input features and the predefined discrete output classes, in order to be able to perform an automatic classification, based on the available data. \n",
    "\n",
    "### Metrics\n",
    "\n",
    "#### Confusion Matrix \n",
    "\n",
    "In order to measure the accuracy of a classifier we use a confusion matrix.  \n",
    "\n",
    "A confusion matrix $\\mathbf{C}$ is a $k \\times k$ matrix containing the classifications statistics of a given classifier.\n",
    "$c_{ij}$ containes the number of times that a sample belonging to the actual class $j$ has been predicted as belonging to class $i$.\n",
    "\n",
    "In our two-class setting, the confusion matrix (reporting the number of actual class / predicted class) has four entries:\n",
    " \n",
    "|            | Actual Negative   | Actual Positive  |\n",
    "|:----------:|-------------------|-----------------|\n",
    "|**Classified Negative** | $T_N$ (True Negative) | $F_N$ (False Negative)| \n",
    "|**Classified Positive** | $F_P$ (False Positive) | $T_P$ (True Positive) |\n",
    "\n",
    "Ideally, if the classifier didn't make any mistake, the confusion matrix $\\mathbf{C}$ should be diagonal.\n",
    "By looking at the off-diagonal elements, we can understand which kind of mistakes the classifier is making (e.g. Actual Negative -> Predicted Positive, Actual Positive -> Predicted Negative).\n",
    "\n",
    "Additionally, the confusion matrix allows to compute the total number of elements classified negative $\\hat{N_N}$, classified positive $\\hat{N_P}$, actual negative $N_N$, actual positive $N_P$, as well as the total number of samples $N$.\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{N_N} = T_N + F_N & & & \\hat{N_P} = T_P + F_P \\\\ \n",
    "N_N = T_N + F_P & & & N_P = T_P + F_N \\\\\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{equation}\n",
    "N = T_N + F_N + T_P + F_P \n",
    "\\end{equation}\n",
    "\n",
    "The quantities in the confusion matrix are used to define different accuracy measures, such as:\n",
    "\n",
    "##### Accuracy\n",
    "\n",
    "\\begin{equation}\n",
    "A = \\frac{T_P + T_N}{N} = \\frac{T_P + T_N}{F_P + F_N + T_P + T_N} \n",
    "\\end{equation}\n",
    "\n",
    "The accuracy represents the ratio between the number of correctly classified samples (False ...) and the total number of samples.\n",
    "\n",
    "##### Misclassification Rate\n",
    "\n",
    "\\begin{equation}\n",
    "ER = \\frac{F_P + F_N}{N} = \\frac{F_P + F_N}{F_P + F_N + T_P + T_N} \n",
    "\\end{equation}\n",
    "\n",
    "The misclassification rate represents the number of total classification mistakes (False ...) over the total number of samples.\n",
    "It can be shown that ER=1-A.\n",
    "\n",
    "##### Balanced Error Rate (BER)\n",
    "\n",
    "\\begin{equation}\n",
    "BER = \\frac{1}{2}(\\frac{F_P}{N_N} + \\frac{F_N}{N_P}) = \\frac{1}{2}(\\frac{F_P}{T_N + F_P} + \\frac{F_N}{T_P + F_N}) \n",
    "\\end{equation}\n",
    "\n",
    "The balanced error rate is computed as an average of the error for each classes.\n",
    "For unbalanced classification problems (i.e. $N_N \\neq N_P$ the total number of samples belonging to the negative and positive classes is different).  \n",
    "\n",
    "The **BER is handy when dealing with unbalanced classes to predict**. For instance, if we have 1000 observations, and classify 500 in the first class and 500 in the other class, we have no issues with the accuracy as we will have 50% of classifying correctly in case we classify randomly. However, if we know from previous data that 900 obs where labeled in one class and 100 in another one, the accuracy measure might not be enough to understand/measure how well the model performs (i.e. that we are dealing with unbalanced classes to predict). We could simply classify all the new obs in the class that had 900 obs and we would have roughly 90% accuracy with that kind of 'dumb' classification. --> The Balanced Error Rate would highlight that our classification model is 'dumb'.\n",
    "\n",
    "##### Sensitivity and specificity\n",
    "\n",
    "These two elements help understand the type of errors the classifier makes.\n",
    "\n",
    "\\begin{align}\n",
    "SE = \\frac{T_P}{N_P} = \\frac{T_P}{T_P + F_N} & & & 0 \\leq SE \\leq 1 \n",
    "\\end{align}\n",
    "\n",
    "The sensitivity (also called recall) is a measure defined as the ratio between the correctly classified positive samples over the total number of positive samples (i.e True Positive rate). It measures the impact of false negatives on the classification process.\n",
    "\n",
    "\\begin{align}\n",
    "SP = \\frac{T_N}{N_N} = \\frac{T_N}{F_P + T_N}  & & & 0 \\leq SP \\leq 1\n",
    "\\end{align}\n",
    "\n",
    "The specificity is a measure defined as the ratio between the correctly classified negative samples over the total number of negative samples (i.e True Negative rate). It measures the impact of false positive on the classification process.\n",
    "\n",
    "The need for different accuracy measures arises from the fact that the impact of a certain type of error (e.g. Actual Negative -> Predicted Positive, Actual Positive -> Predicted Negative) might be greatly different, according to the context in which the classification problem is performed (for example medical diagnosis, fraud detection). One typical example of that is the medical cancer diagnosis. We would probably prefer to diagnose cancer to someone who has actually no cancer (false positive) than not finding/diagnosing cancer while the person actually has cancer (false negative)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to understand how a logistic regression model works, which is a classification model and NOT a regression, we must first understand how a linear regression works.\n",
    "\n",
    "### Linear Regression\n",
    "\n",
    "Interesting reading: https://otexts.com/fpp2/regression-intro.html \n",
    "\n",
    "Let $Y$ be a $n$-random vector, with $E[Y]=\\mu$  \n",
    "\n",
    "In order to have a linear model of dimension $p$, we set an hypothesis on $\\mu$ of the form:\n",
    "$$H_0: \\mu \\in L$$ \n",
    "where $L$ is a linear subspace of $\\mathbb{R}^{n}$ with $dim(L)=k$\n",
    "\n",
    "In other words: $H_0: E[Y] $ is part of a **linear subspace** ($L$) of $\\mathbb{R}^{n} $\n",
    "\n",
    "For a given basis $X=(x_1, ... , x_p)$ of $L$ this means that under $H_0$ there is a coefficient vector $\\beta$ such that:\n",
    "$$E[Y]=\\mu = X\\beta $$\n",
    "$$\\Leftrightarrow Y = X\\beta + W $$\n",
    "\n",
    "Where the $w_i$ are iid realisations of a random variable $\\mathbf{w}$ with:\n",
    " - $E[\\mathbf{W}]=0$ (null mean assumption) \n",
    " - $Var[\\mathbf{W}]=\\sigma^2_{\\mathbf{w}}I_N$: constant variance (**homoscedasticity assumption**). In other words its variance $\\sigma^2_{\\mathbf{w}}$ is independent of the x value. \n",
    "\n",
    "and where:\n",
    "- $Y$ is the $[N × 1]$ response vector, \n",
    "- $X$ is the $[N × p]$ data matrix, whose $j^{th}$ column of $X$ contains readings on the $j^{th}$ regressor,\n",
    "- $\\beta$ is the $[p × 1]$ vector of parameters\n",
    "\n",
    "Visually:\n",
    "\n",
    "\\begin{equation}\n",
    " Y = \\begin{bmatrix}\n",
    " y_1 \\\\\n",
    " y_2 \\\\\n",
    " . \\\\\n",
    " . \\\\\n",
    " . \\\\\n",
    " y_N\n",
    " \\end{bmatrix},\n",
    " X = \\begin{bmatrix}\n",
    " 1 & x_{11} & x_{12} & ... & x_{1p} \\\\\n",
    " 1 & x_{21} & x_{22} & ... & x_{2p} \\\\\n",
    " . & .      & .      & .   & . \\\\\n",
    " . & .      & .      & .   & . \\\\\n",
    " . & .      & .      & .   & . \\\\\n",
    " 1 & x_{N1} & x_{N2} & ... & x_{Np} \\\\\n",
    " \\end{bmatrix} =\n",
    " \\begin{bmatrix}\n",
    " x_{1}^T \\\\\n",
    " x_{2}^T \\\\\n",
    " . \\\\\n",
    " . \\\\\n",
    " . \\\\\n",
    " x_{N}^T \\\\\n",
    " \\end{bmatrix},\n",
    " \\beta = \\begin{bmatrix}\n",
    " \\beta_{0} \\\\\n",
    " \\beta_{1} \\\\\n",
    " . \\\\\n",
    " . \\\\\n",
    " . \\\\\n",
    " \\beta_{p} \\\\\n",
    " \\end{bmatrix},\n",
    " W = \\begin{bmatrix}\n",
    " w_{1} \\\\\n",
    " w_{2} \\\\\n",
    " . \\\\\n",
    " . \\\\\n",
    " . \\\\\n",
    " w_{N} \\\\\n",
    " \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "In more general terms, it is assumed under $H_0$ that: $Y$ is a linear combination of $X$ variables, and an error term.\n",
    "\n",
    "Consider a linear relation between an independent variable $x\\in X \\subset \\mathbb{R}^n$ and a dependent random variable $y \\in Y \\subset \\mathbb{R}$.\n",
    "We can rewrite the above formula as:\n",
    "$$y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_nx_n + w$$\n",
    "and in matrix notation:\n",
    "$$y=x^T\\beta+w $$\n",
    "where:\n",
    "- $x$: $[p x 1]$ vector : $x = [1, x_{·1}, x_{·2}, . . . , x_{·n}]^T$\n",
    "- $\\beta = [\\beta_0,...,\\beta_n]^T$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Example: univariate linear regression\n",
    "Let's take the simplest regression model as example: the univariate linear regression model. The input is supposed to be a scalar variable and the stochastic dependency between input and output is described by\n",
    "\n",
    "\\begin{equation*}\n",
    " y=\\beta_0+\\beta_1 x +w\n",
    "\\end{equation*}\n",
    "\n",
    "In practice, we will assume to have $N$ observation pairs $D_N =\\{(x_i,y_i)\\}$ generated by the following stochastic process:\n",
    "\n",
    "\\begin{equation*}\n",
    " y_i=\\beta_0+\\beta_1 x_i +w_i,\n",
    "\\end{equation*}\n",
    "\n",
    "where:  \n",
    "- $x \\in \\mathbb{R}$ is the regressor (or independent) variable\n",
    "- $y$ is the measured response (or dependent) variable, \n",
    "- $\\beta_0$ is the intercept, \n",
    "- $\\beta_1$ is the slope,\n",
    "- $w$ is called noise or model error. \n",
    "\n",
    "with the following assumptions:  \n",
    "\n",
    "the $w_i$ are iid realisations of a random variable $\\mathbf{w}$ with:\n",
    " - $\\mu_{\\mathbf{w}}=E[\\mathbf{w}]=0$ (null mean assumption) \n",
    " - $Var[\\mathbf{w}]=\\sigma^2_{\\mathbf{w}}$: constant variance. In other words its variance $\\sigma^2_{\\mathbf{w}}$ is independent of the x value. The assumption of constant variance is often referred to as **homoscedasticity**.\n",
    "\n",
    "In practice, the $x_i$ can be seen as *fixed*, the only random component in the sample set $D_N$ is therefore contained in the $y_i$ (which are random due to the $w_i$).\n",
    "\n",
    "In machine learning, the goal is to predict $Y$ based on some inputs $X$, and therefore we obtain:\n",
    "$$Prob(Y=Y|X)=Prob(W=Y−X\\beta)$$ \n",
    "$$E[y|x]=f(x)= X\\beta$$\n",
    "In the univariate case, this gives: $Prob(y=y|x)=Prob(w=y−\\beta_0 − \\beta_1 x), E[y|x]=f(x)=\\beta_0 +\\beta_1 x$\n",
    "\n",
    "The function $f(x) = E[y|x]$, also known as regression function, is a linear function in the parameters $\\beta (\\beta_0, \\beta_1, ...)$. In the following we will intend as linear model each input/output relationship which is linear in the parameters but not necessarily in the dependent variables. This means that: i) any value of the response variable y is described by a linear combination of a series of parameters (regression slopes, intercept) and ii) no parameter appears as an exponent or is multiplied or divided by another parameter.\n",
    "\n",
    "\n",
    "#### Estimation of the parameters\n",
    "\n",
    "Now that we know $$E[Y]=\\mu = X\\beta \\Leftrightarrow Y = X\\beta + W, $$\n",
    "we need to estimate $\\beta$ with $\\hat{\\beta}$ and $\\sigma^2$ (from $Var[w]=\\sigma^2I_n$) with $\\hat{\\sigma^2}$.\n",
    "\n",
    "In practice, of course, we have a collection of observations but we do not know the values of the coefficients $\\beta$ ($\\beta_0, \\beta_1, \\beta_2, ..., \\beta_p$). These need to be estimated from the data.\n",
    "\n",
    "These parameters can be estimated by both Ordinary Least Squares (OLS) or Maximum Likelihood (MLE). In fact, it can be shown that both methods yield the same result. \n",
    "The MLE initially requires the assumption that $Y \\sim N_n(\\mu,\\sigma^2I_n)$, which makes it less intuitive. However, following the Gauss Markov theorem (BLUE - Best Linear Unbiased Estimator), it can be shown that the computed estimators by MLE also make sense when $Y$ is not normally distributed.  \n",
    "\n",
    "\n",
    "##### Estimation by Least Squares (OLS)\n",
    "\n",
    "The least squares principle provides a way of choosing the coefficients effectively by minimising the sum of the squared errors. That is, we choose the values of $\\beta_0, \\beta_1,..., \\beta_p$ that minimise the error term $w$:  \n",
    "\n",
    "$$\\arg \\min_{\\beta_0, \\beta_1,..., \\beta_p} \\sum \\limits _{i=1} ^{N} w_i ^2 = \\arg \\min_{\\beta_0, \\beta_1,..., \\beta_p} \\sum \\limits _{i=1} ^{N}(y_i − \\hat{y_i})^2 =  \\arg \\min_{\\beta_0, \\beta_1,..., \\beta_p} \\sum \\limits _{i=1} ^{N}(y_i − \\hat{\\beta_0} - \\hat{\\beta_1}x_{1,i} - \\hat{\\beta_2}x_{2,i} - ... - \\hat{\\beta_p}x_{p,i} )^2 $$\n",
    "\n",
    "This is called least squares estimation because it gives the least value for the sum of squared errors. Finding the best estimates of the coefficients is often called “fitting” the model to the data, or sometimes “learning” or “training” the model.\n",
    "\n",
    "Formulated differently, the least-squares estimator $\\hat{\\beta}$ is such that:\n",
    "\n",
    "$$\\hat{\\beta} = \\arg \\min_b \\sum \\limits _{i=1} ^{N} (y_i − x_{i}^T b)^2 = \\arg \\min_b ( (Y − Xb)^T (Y − Xb) )  $$\n",
    "\n",
    "Given $\\hat{\\beta}$ we obtain:\n",
    "$$SSE_{empirical}= ((Y-X\\hat{\\beta})^T(Y-X\\hat{\\beta}))=e^Te$$\n",
    "where:\n",
    "- $SSE_{empirical}$ represents the residual sum of squares for linear models\n",
    "- $e$ is the $[N x 1]$ vector of residuals\n",
    "\n",
    "The empirical (or training) error quantity:\n",
    "$$\\hat{MISE_{emp}} = \\frac{SSE_{emp}}{N} $$\n",
    "\n",
    "When computing, the vector $\\hat{\\beta}$ must satisfy:\n",
    "$$\\frac{\\partial}{\\partial \\hat{\\beta}}[(Y-X\\hat{\\beta})^T(Y-X\\hat{\\beta})] = 0$$ \n",
    "$$\\Leftrightarrow -2X^T(Y-X\\hat{\\beta}) = 0 $$\n",
    "$$\\Leftrightarrow ...$$\n",
    "$$\\Leftrightarrow \\hat{\\beta}= (X^TX)^{-1}X^TY$$\n",
    "where the $X^TX$ matrix is a positive definite symmetric $[p x p]$ matrix which plays an important role in multiple linear regression.\n",
    "\n",
    "We can then rewrite the expression as:\n",
    "$$\\hat{Y} = X\\hat{\\beta} = X(X^TX)^{-1}X^TY$$\n",
    "\n",
    "\n",
    "##### Properties of the Least Squares estimate $\\hat{\\beta}$\n",
    "\n",
    "If the linear dependency assumption holds then:\n",
    "- $E_{D_N}[\\hat{\\beta}]=\\beta$: if $E[w] = 0$ then $\\hat{\\beta}$ is an unbiased estimator of $\\beta$  \n",
    "- the *residual mean square* estimator $\\hat{\\sigma}^2_{\\mathbf{w}}=\\frac{\\sum_{i=1}^N(y_i-\\hat{y_i})^2}{N-p}=\\frac{(Y-X\\hat{\\beta})^T(Y-X\\hat{\\beta})}{N-p}$ is an unbiased estimator of of the error variance $\\sigma_w^2$\n",
    "- If the $w_i$ are uncorrelated and have common variance, the variance-covariance matrix of $\\hat{\\beta}$ is given by:\n",
    "$Var[\\hat{\\beta}]=\\sigma^2_w(X^TX)^{-1}$\n",
    "It can also be shown (Gauss-Markov theorem) that the least-squares estimation $\\hat{\\beta}$ is the ”best linear unbiased estimator” (BLUE) i.e. it has the lowest variance among all linear unbiased estimators.\n",
    "\n",
    "\n",
    "##### Variance of the prediction and confidence intervals\n",
    "\n",
    "\n",
    "Since the estimator $\\hat{\\beta}$ is unbiased, this is also the case for the the prediction $\\hat{y} = x^T_0 \\hat{\\beta}$ for a generic input value $x = x_0$. Its variance is:\n",
    "$$Var[\\hat{y}|x_0] = \\sigma_w^2 x_0^T(X^TX)^{-1}x_0$$\n",
    "\n",
    "From the results above it is possible to derive the confidence intervals of model parameters:  \n",
    "**Assuming that $w$ is normally distributed**, the $100(1−α)\\%$ confidence bound for the\n",
    "regression value $E[\\hat{y}|x=x_0]$ is given by:\n",
    "$$\\hat{y}(x_0)\\pm t_{\\alpha/2, N-p}\\hat{\\sigma}_w \\sqrt{x_0^T(X^TX)^{-1}x_0}$$\n",
    "\n",
    "where:\n",
    "- $t_{\\alpha/2, N-p}$ is the upper $\\alpha/2$ percent point of the t-distribution with $N-p$ degrees of freedom\n",
    "- the quantity $\\hat{\\sigma}_w \\sqrt{x_0^T(X^TX)^{-1}x_0}$ is the *standard error of prediction* for multiple regression.\n",
    "\n",
    "\n",
    "##### Evaluating the prediction: Generalization error of the linear model\n",
    "\n",
    "Now that we have computed estimates for the linear regression model, we can use those estimates to predict for a given input $x$ the future output $y(x)$.\n",
    "The future output $y(x)$ is independent of the training set $D_N$.\n",
    "\n",
    "Given a training dataset $D_N = {⟨xi,yi⟩ : i = 1,...,N}$ and a query point $x$, it is possible to return a linear prediction\n",
    "$$\\hat{y} = h(x, \\alpha) = x^T\\hat{\\beta}$$\n",
    "\n",
    "where $\\beta$ is returned by least-squares estimation (as seen above).\n",
    "\n",
    "When using estimates for estimation of a model, one question arises: which precision can we expect from $\\hat{y} = x^T\\hat{\\beta}$ if we average the prediction error over all finite-size datasets $D_N$ that can be generated by the linear dependency?\n",
    "\n",
    "A quantitative measure of the quality of the linear predictor on the whole domain $X$ is the Mean Integrated Squared Error (MISE). But: \n",
    "- how can we estimate this quantity in the linear case? \n",
    "- is the empirical risk $\\hat{MISE}_{emp}$ a reliable estimate of $MISE$?\n",
    "\n",
    "\n",
    "###### Mean Squared Error MSE\n",
    "\n",
    "A measure of error is the Mean Squared Error (MSE): \n",
    "$$MSE(x) = E_{D_N,y}[(y(x)-x^T\\hat{\\beta})^2]= \\sigma_w^2 + E_{D_N}[(x^T\\beta-x^T\\hat{\\beta})^2] $$\n",
    "where $y(x_i)$ is independent of $D_N$ and the integrated version:\n",
    "$$MISE=\\int_x MSE(x)p(x)dx$$\n",
    "\n",
    "\n",
    "###### The expected empirical error, a biased estimate of the MISE\n",
    "\n",
    "Here we show that the empirical risk $\\hat{MISE_{emp}}$ is a biased estimate of the MISE generalisation error.\n",
    "\n",
    "We have defined above the empirical (or training) error quantity as: $$\\hat{MISE_{emp}} = \\frac{SSE_{emp}}{N}$$ where\n",
    "$SSE_{empirical}= ((Y-X\\hat{\\beta})^T(Y-X\\hat{\\beta}))=e^Te$\n",
    "\n",
    "and therefore we can rewrite:\n",
    "\n",
    "$$\\hat{MISE_{emp}} = \\frac{SSE_{emp}}{N} = \\frac{((Y-X\\hat{\\beta})^T(Y-X\\hat{\\beta}))}{N} = \\frac{e^Te}{N}$$\n",
    "\n",
    "The expectation of the residual sum of squares can be written as:\n",
    "$$\n",
    "E_{D_N}[\\hat{MISE_{emp}}] = E_{D_N} \n",
    "\\left[ \n",
    "\\frac{\\sum \\limits _{i=1} ^{N} (y_i − x_{i}^T \\hat{\\beta})^2}{N}\n",
    "\\right]\n",
    "= \\frac{N-p}{N} E_{D_N}\n",
    "\\left [\\frac{\\sum \\limits _{i=1} ^{N} (y_i − x_{i}^T \\hat{\\beta})^2}{N-p} \\right]\n",
    "= \\frac{N-p}{N} \\sigma_w ^2\n",
    "$$\n",
    "This is the expectation of the error made by a linear model **trained on $D_N$** to predict the value of the **output in $D_N$**.\n",
    "\n",
    "\n",
    "Now, let us compute now the expected prediction error of a linear model **trained on $D_N$** when this is used to predict a set of outputs distributed according to the same linear dependency but **independent of the training set $D_N$** (-> $y$ is independent of the training set!).\n",
    "\n",
    "It can be shown that in case of linear dependency:\n",
    "$$MISE = E_{D_N,y} \n",
    "\\left[\n",
    "\\frac{\\sum \\limits _{i=1} ^{N} (y − x_{i}^T \\hat{\\beta})^2}{N}\n",
    "\\right]\n",
    "= \\frac{N+p}{N} \\sigma_w^2\n",
    "$$\n",
    "\n",
    "\n",
    "We can clearly see that the **empirical error returns a biased estimate of MISE !**\n",
    "\n",
    "Therefore, if we replace $\\hat{MISE_{emp}}$ by $$\\hat{MISE_{emp}}+2\\frac{p}{N}\\sigma_w^2$$\n",
    "we obtain an unbiased estimator of the quantity MISE.\n",
    "Nevertheless, this estimator requires an estimate of the noise variance.\n",
    "\n",
    "###### Predicted Square Error (PSE) and Final Prediction Error (FPE)\n",
    "\n",
    "Given an a priori estimate $\\hat{\\sigma}_w^2$ we have the Predicted Square Error (PSE) criterion:\n",
    "$$PSE = \\hat{MISE_{emp}} + 2\\frac{p}{N}\\hat{\\sigma}_w^2$$\n",
    "\n",
    "Taking as estimate of $\\hat{\\sigma}_w^2$:\n",
    "$$\\hat{\\sigma}_w^2 = \\frac{1}{N-p}SSE_{emp}$$\n",
    "\n",
    "we have the **Final Prediction Error FPE**:\n",
    "\n",
    "$$FPE = \\frac{1+\\frac{p}{N}}{1-\\frac{p}{N}} \\hat{MISE_{emp}}\n",
    "$$\n",
    "\n",
    "The PSE and the FPE criteria allow us to replace the empirical risk with a more accurate estimate of the generalisation error of a linear model. Although their expression is easy to compute, it is worth reminding that their derivation relies on the assumption that the stochastic input/output dependence has the linear form.\n",
    "\n",
    "\n",
    "###### Goodness of fit: Squared Residuals ($R^2$)\n",
    "\n",
    "Another method often used to evaluate the fitting of the linear model is the $R^2$. But it is often misused.\n",
    "$$R^2 = \\frac{\\sum \\limits ^N _{i=1} (\\hat{y}_i - \\bar{y})^2}{\\sum \\limits ^N _{i=1} (y_i - \\bar{y})^2}$$\n",
    "\n",
    "The denominator indicates the total variation (from the average) while the numerator indicates the variation that is accounted for by the linear model. Hence, the $R^2$ indicates a proportion of the total variation that is explained by the fit of the linear model.\n",
    "\n",
    "We have $R^2 \\in [0;1]$  \n",
    "\n",
    "If the linear model's prediction is close to the actual values, then the $R^2$ should be close to 1.  \n",
    "If the linear model's prediction is completely different thant the actual values, then the $R^2$ should be close to 0.\n",
    "\n",
    "IMPORTANT: $R^2$ will NEVER decrease when adding an extra predictor, which incentivizes to add as many predictors as possible. This then leads to OVERFITTING.\n",
    "\n",
    "Hence, validating a model's forecasting performance on the test data is much better than measuring the $R^2$ value on the training data!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example: Estimating univariate linear regression's $\\beta$ by Least Squares (OLS)\n",
    "\n",
    "The coefficients of $\\beta_0$ and $\\beta_1$ can be estimated using the least squares method. This method consists of taking those estimators $\\hat{\\beta_0}$ and $\\hat{\\beta_1}$ which minimize the error term:\n",
    "\n",
    "\\begin{equation}\n",
    " R_{emp}=\\sum_{i=1}^N (y_i-\\hat{y_i})^2\n",
    "\\end{equation}\n",
    "\n",
    "where $\\hat{y_i}=\\hat{\\beta_0}+\\hat{\\beta_1} x_i$.\n",
    "\n",
    "This is equivalent to \n",
    "\n",
    "\\begin{equation}\n",
    " \\{\\hat{\\beta_0},\\hat{\\beta_1}\\}=\\arg\\min_{b_0,b_1}\\sum_{i=1}^N (y_i-b_0-b_1x_i)^2.\n",
    "\\end{equation}\n",
    "\n",
    "The solution is given by *(Eq. 1)*\n",
    "\n",
    "\\begin{equation}\n",
    " \\hat{\\beta_1}=\\frac{S_{xy}}{S_{xx}},\\quad \\hat{\\beta_0}=\\bar{y}-\\hat{\\beta_1}\\bar{x},  \n",
    "\\end{equation}\n",
    "\n",
    "Also, the $\\beta_1$ coefficient can be rewritten to highlight his dependency on the empirical correlation coefficient $\\hat{\\rho_{xy}}$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\beta_1}=\\frac{S_{xy}}{S_{xx}} = \\sqrt{\\frac{S_{yy}}{S_{xx}}}*\\hat{\\rho_{xy}}\n",
    "\\end{equation}\n",
    "\n",
    "where *(Eq. 2)*\n",
    "\n",
    "\\begin{equation}\n",
    " \\bar{x}=\\frac{\\sum_{i=1}^Nx_i}{N},\\quad\\bar{y}=\\frac{\\sum_{i=1}^Ny_i}{N},\\quad S_{xy}=\\sum_{i=1}^N (x_i-\\bar{x})y_i,\\quad S_{xx}=\\sum_{i=1}^N(x_i-\\bar{x})^2.\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "#### Properties of the estimator\n",
    "\n",
    "* $E_{D_N}[\\hat{\\beta_1}]=\\beta_1$\n",
    "* $Var[\\hat{\\beta_1}]=\\frac{\\sigma^2}{S_{xx}}$\n",
    "* $E[\\hat{\\beta_0}]=\\beta_0$\n",
    "* $Var[\\hat{\\beta_0}]=\\sigma^2\\left( \\frac{1}{N}+\\frac{\\bar{x}^2}{S_{xx}}\\right)$\n",
    "* $\\hat{\\sigma}^2_{\\mathbf{w}}=\\frac{\\sum_{i=1}^N(y_i-\\hat{y_i})^2}{N-2}$ is a non-biased estimator of $\\sigma^2_{\\mathbf{w}}$.\n",
    "\n",
    "\n",
    "#### Partitioning the variability\n",
    "\n",
    "The variability of the response $y_i$ can be expressed as follows\n",
    "\n",
    "\\begin{equation}\n",
    " \\sum_{i=1}^N(y_i-\\bar{y})^2=\\sum_{i=1}^N(\\hat{y_i}-\\bar{y})^2+\\sum_{i=1}^N(y_i-\\hat{y_i})^2,\n",
    "\\end{equation}\n",
    "\n",
    "that is\n",
    "\n",
    "\\begin{equation}\n",
    " SS_{tot}=SS_{mod}+SS_{res}.\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "We can now better understand the logistic regression.\n",
    "\n",
    "Whereas a logistic regression is a regression model (we try to predict a numerical $y$ variable), the logistic regression is a classification model (we try to predict the correct class $y$ belongs to).\n",
    "\n",
    "When dealing with classifications, several techniques can be used to adapt regression techniques to classifications:\n",
    "- **Direct estimation via regression techniques:**  \n",
    "\n",
    "If the classification problem has $K = 2$ classes and if we denote them by $y = 0$ and $y = 1$ then:\n",
    "$$E[y|x]=1*Prob\\{y=1|x\\} + O*Prob\\{y=0|x\\} = Prob\\{y=1|x\\}$$\n",
    "Then the classification problem can be put in the form of a regression problem where the output takes value in $\\{0, 1\\}$.  \n",
    "However, this means that, in principle, all the regression techniques presented so far could be used to solve a classification task. In practice, most of those techniques do not make any assumption about the fact that the outcome in a classification task should satisfy the probabilistic constrains, e.g. $0 ≤ Prob \\{y = 1|x\\} ≤ 1$. This means that only some regression algorithms (e.g. local constant models) are commonly used for binary classification as well. One famous model is the K-NN.\n",
    "\n",
    "- **Direct estimation via cross-entropy** \n",
    "\n",
    "The approach consists in modelling the conditional distribution $Prob\\{y = c_j |x\\} , j = 1,...,K$ with a set of models $\\hat{P}_j(x,\\alpha),j = 1,...,K$ satisfying the constraints $\\hat{P}_j (x, \\alpha) > 0$ (positive probabilities) and $\\sum \\limits ^K _{j=1} \\hat{P}_j(x, \\alpha) = 1 $ (sum of probabilities equal to 1)\n",
    "\n",
    "This is a parametric approach where we try to find the $\\alpha$ parameter(s). Parametric estimation is done by minimisation of the cross-entropy cost function or by maximum likelihood.\n",
    "\n",
    "The cross-entropy is defined by:\n",
    "$$\n",
    "J(\\alpha) = -\\log \\prod \\limits ^{N} _{i=1} Prod\\{y=y_i|x_i\\}\n",
    " = - \\sum \\limits ^N _{i=1} \\log \\hat{P}_{y_i}(x_i,\\alpha)$$\n",
    "\n",
    "One typical cross-entropy approach is the logistic regression.  \n",
    "In logistic regression for a two-class task we have:\n",
    "\n",
    "$$\n",
    "\\hat{P}_1(x,\\alpha) = \\frac{\\exp^{x^T\\alpha}}{1+\\exp^{x^T\\alpha}} = \\frac{1}{1+\\exp^{-x^T\\alpha}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{P}_2(x,\\alpha) = \\frac{1}{1+\\exp^{x^T\\alpha}} \n",
    "$$\n",
    "\n",
    "which implies:\n",
    "$$\n",
    "\\log \\frac{\\hat{P}_1(x,\\alpha)}{\\hat{P}_2(x,\\alpha)} = x^T\\alpha\n",
    "$$\n",
    "where the transformation $\\frac{p}{1-p}$ is known as the *logit* transformation.\n",
    "\n",
    "In a binary case $(c_1=1,c_2=0)$ the cross-entropy function becomes\n",
    "$$\n",
    "J(\\alpha) = - \\sum \\limits ^N _{i=1} \\log \\hat{P}_{y_i}(x_i,\\alpha) =\n",
    "- \\sum \\limits ^N _{i=1} \\{ y_i \\log \\hat{P}_1(x_i,\\alpha) + (1-y_i)\\log \\hat{P}_1(x_i,\\alpha) \\}\n",
    "= - \\sum \\limits ^N _{i=1} \\{ y_i x_i ^T \\alpha - \\log(1+ \\exp ^{x_i ^T \\alpha}) \\}\n",
    "$$\n",
    "\n",
    "Given the nonlinear nature of the task an iterative solution (iteratively reweighted least squares) is required.\n",
    "\n",
    "- **Density estimation via Bayes theorem**\n",
    "\n",
    "- **Discriminant functions**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survived\n",
      "[0, 1]\n",
      "---------\n",
      "Pclass\n",
      "[1, 2, 3]\n",
      "---------\n",
      "Sex\n",
      "['female', 'male']\n",
      "---------\n",
      "Embarked\n",
      "['C', 'Q', 'S']\n",
      "---------\n"
     ]
    }
   ],
   "source": [
    "# Check columns that need to be one-hot encoded\n",
    "for column in train_set.columns:\n",
    "    if (train_set[column].dtype.name == 'category'):\n",
    "        print(column)\n",
    "        print(list(train_set[column].cat.categories.values))  # list(train_set[column].unique())\n",
    "        print(\"---------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding\n",
    "encoded_train_set = pd.get_dummies(data = train_set, columns = [\"Embarked\",\"Pclass\",\"Sex\"], drop_first=True) \n",
    "encoded_test_set = pd.get_dummies(data = test_set, columns = [\"Embarked\",\"Pclass\",\"Sex\"], drop_first=True)\n",
    "# we specify columns to be one-hot encoded. However, documentation says that columns=None \n",
    "# will one-hot encode all Object or category dtype\n",
    "#encoded_train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative: create new column, then append it to the original dataframe (axis=1 is VERY important)\n",
    "'''\n",
    "sex = pd.get_dummies(train_set['Sex'], drop_first=True) # activate drop_first such that we don't have redundant columns.\n",
    "embark = pd.get_dummies(titanic['Embarked'], drop_first=True)\n",
    "\n",
    "titanic_full = pd.concat([titanic,sex,embark], axis=1) # need to concatenate original titanic set with the new dummy columns\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId: int64\n",
      "Survived: category\n",
      "Survived\n",
      "[0, 1]\n",
      "---------\n",
      "Name: string\n",
      "Age: float64\n",
      "SibSp: int64\n",
      "Parch: int64\n",
      "Ticket: string\n",
      "Fare: float64\n",
      "Embarked_Q: uint8\n",
      "Embarked_S: uint8\n",
      "Pclass_2: uint8\n",
      "Pclass_3: uint8\n",
      "Sex_male: uint8\n"
     ]
    }
   ],
   "source": [
    "for column in encoded_train_set.columns:\n",
    "    print(column+\": \"+str(encoded_train_set[column].dtype)) \n",
    "\n",
    "    if (encoded_train_set[column].dtype.name == 'category'):    \n",
    "        print(column)\n",
    "        print(list(encoded_train_set[column].cat.categories.values))  # list(train_set[column].unique())\n",
    "        print(\"---------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables we use in the model:\n",
    "# X: 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked_Q', 'Embarked_S', 'Pclass_2', 'Pclass_3', 'Sex_male'\n",
    "# y: 'Survived'\n",
    "# encoded_train_set.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split train and validation sets\n",
    "\n",
    "We currently have: \n",
    "- ```encoded_train_set``` that is the set containing (x,y) pairs\n",
    "- ```encoded_test_set```that contains the x values and for which we need to predict $\\hat{y}(x)$\n",
    "\n",
    "Hence, we have two options: \n",
    "- split the ```encoded_train_set``` into one set to fit the model (around 70% of the data) and one set to validate the performance of the model (remaining % of the data).\n",
    "- use k-fold cross-validation where the data will be split into k-folds, each time one fold will be used to assess the performance and the other folds to train the model. Hence, the process is repeated k times. Then, we average the performances.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = encoded_train_set[['Age', 'SibSp', 'Parch', 'Fare', 'Embarked_Q', 'Embarked_S', 'Pclass_2', 'Pclass_3', 'Sex_male']]\n",
    "y = encoded_train_set[\"Survived\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METHOD 1 : SPLIT INTO TRAIN AND VALIDATION SETS\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_model, X_validate_model, Y_train_model, Y_validate_model = train_test_split(X, y, test_size = 0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=200)"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lgm = LogisticRegression(solver='lbfgs', max_iter=200)\n",
    "lgm.fit(X_train_model, Y_train_model)   # we train/fit the model on the training X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.87      0.84       157\n",
      "           1       0.79      0.71      0.75       111\n",
      "\n",
      "    accuracy                           0.80       268\n",
      "   macro avg       0.80      0.79      0.79       268\n",
      "weighted avg       0.80      0.80      0.80       268\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict for new X inputs (from the validation set)\n",
    "predictions = lgm.predict(X_validate_model)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(Y_validate_model, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theory about these different values from the course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using k-fold cross-validation\n",
    "Let's take k=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select variables for the model\n",
    "X = encoded_train_set[['Age', 'SibSp', 'Parch', 'Fare', 'Embarked_Q', 'Embarked_S', 'Pclass_2', 'Pclass_3', 'Sex_male']]\n",
    "y = encoded_train_set[\"Survived\"]\n",
    "\n",
    "# create model\n",
    "model = LogisticRegression(solver='lbfgs', max_iter=200)\n",
    "\n",
    "# prepare the cross-validation procedure\n",
    "cv_10 = KFold(n_splits=10, random_state=1, shuffle=True)\n",
    "\n",
    "# evaluate model\n",
    "scores = cross_val_score(model, X=X, y=y, scoring='accuracy', cv=cv_10, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.804 (std dev: 0.035)\n"
     ]
    }
   ],
   "source": [
    "#print(scores)\n",
    "# Performance\n",
    "print('Accuracy: %.3f (std dev: %.3f)' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of the 10-fold cross-validation is the same as for the 70-30 train-validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/33468566/how-to-select-rows-based-categories-in-pandas-dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're happy with the model, let's predict on the encoded_test_set that contains only the X (and y has to be predicted)\n",
    "\n",
    "X_test = encoded_test_set[['PassengerId','Age', 'SibSp', 'Parch', 'Fare', 'Embarked_Q', 'Embarked_S', 'Pclass_2', 'Pclass_3', 'Sex_male']]\n",
    "\n",
    "\n",
    "# Input data for the training phase\n",
    "X_train = encoded_train_set[['Age', 'SibSp', 'Parch', 'Fare', 'Embarked_Q', 'Embarked_S', 'Pclass_2', 'Pclass_3', 'Sex_male']]\n",
    "y_train = encoded_train_set[\"Survived\"]\n",
    "\n",
    "#create model\n",
    "logistic_regr = LogisticRegression(solver='lbfgs', max_iter=200)\n",
    "logistic_regr.fit(X=X_train , y=y_train) #fit the model\n",
    "\n",
    "# use fitted model to predict given input X_test\n",
    "prediction = logistic_regr.predict(X_test.loc[ : , X_test.columns != 'PassengerId']) #use X_test except the PassengerId column to predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived\n",
       "0          892         0\n",
       "1          893         0\n",
       "2          894         0\n",
       "3          895         0\n",
       "4          896         1"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate the prediction with the PassengerId\n",
    "\n",
    "#Apparently this is the way to do it, I hope rows stay consistent\n",
    "# https://stackoverflow.com/questions/43549034/map-predictions-back-to-ids-python-scikit-learn-decisiontreeclassifier\n",
    "# https://stackoverflow.com/questions/51078742/recovering-instances-ids-after-training-and-prediction\n",
    "\n",
    "submission = pd.DataFrame(data=X_test[\"PassengerId\"])   # We take (the PassengerId column of) X_test\n",
    "submission['Survived'] = prediction                   # We add a column called 'Survived' (as asked on Kaggle)\n",
    "submission.head() #Shows the first few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deliver the output\n",
    "submission.to_csv(\"/Users/guillaumedelande/Documents/Programming/Data Science - Engineering/Training/Titanic dataset/submission_logistic_regression.csv\",index=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier - Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model structure\n",
    "\n",
    "A classification tree is a model employing a tree-based structure in order to perform classification of the target variable. It uses a binary branching structure: the result of each comparison between an observation of $x$ and some feature will be either true or false. It is also sometimes called CART (Classification and Regression Tree).\n",
    "Decision Trees are binary trees and use a divide-and-conquer approach.\n",
    "\n",
    "A decision tree **partitions the input space into mutually exclusive regions**, each of which is assigned a procedure to characterise its data points.\n",
    "\n",
    "The tree structure contains: \n",
    "- Nodes: Each Node represents an attribute (feature)  \n",
    "    - **internal nodes**:  An internal node is a decision-making unit that evaluates a decision function to determine which child node to visit next.\n",
    "    - **terminal nodes** (often called leaves):  A terminal node or leaf has no child nodes and is associated with one of the partitions of the input space. Note that each terminal node has a unique path that leads from the root to itself.\n",
    "- Edges: Each Edge represents a distinct value of that attribute\n",
    "\n",
    "\n",
    "In **classification trees** each terminal node contains a *label* that indicates the class for the associated input region.  \n",
    "\n",
    "In **regression trees** the terminal node contains a model that specifies the input/output mapping for the corresponding input partition.\n",
    "\n",
    "\n",
    "Each internal node is associated to a feature in the input vector $\\mathbf{x}$, and performs a partition of the input space according to the value of the associated feature.\n",
    "Each terminal node is associated to a specific output label $C_i$.\n",
    "**Internal nodes can be concatenated with each other** in order to specify additional partitions of the input space.\n",
    "\n",
    "By combining these definitions, we can notice that the **terminal nodes partitions the input space into mutually exclusive regions** (in a divide-and-conquer fashion). \n",
    "\n",
    "The classification operation is performed by **traversing the tree from the root node until one of the terminal nodes** is reached.\n",
    "At each intermediate internal node, a decision is made according to the value associated to the corresponding feature (for example, for the i-th feature $x_i \\geq 0$).\n",
    "According to the outcome of this decision, a different path in the tree is taken.\n",
    "The **tree traversal is stopped once a terminal node is reached**, and the output of the classifier correspond to the label associated to the terminal node (e.g. $C_0$).\n",
    "\n",
    "Hence, by simply looking at the tree structure, we can easily understand the sequence of operations (i.e. splits) that yielded to the classification decision, improving **interpretability of the model**.\n",
    "A decision tree is thus a natural and simple way of inducing following kind of rules\n",
    "*If [Price is x], [ Occupied is y ] and [ Type is z ], then they will not enter*\n",
    "\n",
    "           \n",
    "It is to note that Decision Trees can express any boolean function of the input attributes: *AND* function, *OR* function, and *XOR* function.\n",
    "\n",
    "\n",
    "### Learning procedure\n",
    "\n",
    "Finding the smallest decision tree is a complex problem (NP-hard). Instead, it relies on greedy heuristics.\n",
    "The idea is to:\n",
    "1. Start from an empty decision tree  \n",
    "2. Pick the best attribute/feature to split the data: see further about tree growing (and minimizing a cost function)  \n",
    "3. Follow the answer path\n",
    "4. Iterate step 2 over leafs\n",
    "\n",
    "Two questions arise: \n",
    "- Best attribute: how to pick the best attribute? What attribute yields the most **pure groups**.\n",
    "- Termination criteria: when does the algorithm stops?\n",
    "\n",
    "\n",
    "As an analogy to their biological equivalent, the learning procedure for a decision tree has two steps known as *tree growing* and *tree pruning*.\n",
    "\n",
    "* During ***tree growing***, an iterative, exhaustive search is performed to find the successive splits, **selecting the one that minimizes a certain cost function**.  \n",
    "In more general terms, we try to find the attribute that will split the data into the most 'pure' groups. For instance, we will want to find the attribute/feature that will split the data into groups that have the same label/target. The $x$ data that have label $y=True$ need to be grouped together and the data with label $y=False$ need to be together. If the attribute/feature splits the data in such a way that $y=True$ observations and $y=False$ observations are completely mixed together, then the attribute performs poorly in the decision tree. \n",
    "Hence, selecting the best attribute/feature on which to split the data is done using a **purity measure**.\n",
    "  \n",
    "  Here you can find some examples of commonly used cost functions (i.e. functions that evaluate a node's impurity), with $p_i$ representing the probability of selecting a sample of class $i$ in the input space subset defined by the considered split:\n",
    "\n",
    "    * The **Entropy**: Entropy is a measure of information that indicates the disorder of the features with the target. It gets its maximum value when the probability of the two classes is the same and a node is pure when the entropy has its minimum value, which is 0.\n",
    "    The lower the entropy the better the attribute for splitting the data. The value of the entropy is in $[0,1]$\n",
    "    A high entropy is characterized by a Uniform distribution (flat histogram) while a low entropy is characterized by a varied distribution (histogram with many highs and lows).\n",
    "    \n",
    "    \\begin{equation}\n",
    "         Entropy = \\sum_{i=1}^C - p_i * log_2 (p_i) \\\\\n",
    "    \\end{equation}\n",
    "    \n",
    "    Example: We have a set S comprised of 8 values (2 negative and 6 positive). We therefore have: $p_1=p(+)=\\frac{6}{8}=0.75$ and $p_2=p(-)=\\frac{2}{8}=0.25$. The Entropy is given by: $Entropy = \\sum_{i=1}^C - p_i * log_2 (p_i) = -0.75*log_2 (0.75) -0.25*log_2(0.25) = 0.81$  \n",
    "    \n",
    "    * The **Gini Impurity** (used in CART) : how often a randomly chosen element would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the subset. In other words, the gini impurity measures the frequency at which any element of the dataset will be mislabelled when it is randomly labeled. It is minimum (zero) for class-homogeneous nodes. The value of the GINI is in $[0, 0.5]$\n",
    "    \n",
    "    \\begin{equation}\n",
    "         Gini = \\sum_{i=1}^C p_i * (1-p_i) = 1-\\sum_{i=1}^C p_i^2\\\\\n",
    "    \\end{equation}\n",
    "    \n",
    "    The GINI Impurity is a bit the same principle as the Entropy. In practice, it often replaces the entropy because is it is computationnally more interesting: no need to compute the logarithm, hence faster training times.\n",
    "    \n",
    "    * The **misclassification error**: see Handbook and slides\n",
    "    * The **Chi-square**:\n",
    "    \n",
    "    \n",
    "    \n",
    "The **information gain** (IG, used in ID3,C4.5) : The entropy (or GINI, or misclassification error, or Chi-Square) only tells us how impure a single set is. Knowing that an attribute will split the set/data into several sets. What we want is to choose the best attribute to split the data --> What we ACTUALLY **need is to measure the decrease in uncertainty** (Remember: high entropy = high uncertainty). Hence, the Information Gain is based on the concept of entropy and information content from information theory. Basically the IG is the **entropy of the parent node minus the weighted sum of the children node entropy**. Hence, we will **select the attribute that maximizes the Information Gain**. \n",
    "\n",
    "\\begin{equation}\n",
    "  IG(parent, children) = Entropy(parent) - \\sum_{j} p(child_j) * entropy(child_j) \\\\\n",
    "  IG = -\\sum_{i=1}^C p_i * log_2 (p_i) - \\sum_{a} p(a) \\sum_{i=1}^C Pr(i|a) * log_2 (Pr(i|a))  \\\\\n",
    "\\end{equation}\n",
    "    \n",
    "* During ***tree pruning***, some branches of the decision tree are removed, based on a complexity based measure of the tree performance, in order to avoid overfitting (see comments below about risk of overfitting).\n",
    "The tree that the growing procedure yields is typically too large and presents a serious risk of overfitting the dataset. For that reason, a pruning procedure is often adopted.\n",
    "\n",
    "\n",
    "\n",
    "**Termination criteria**: when do we stop growing the tree\n",
    "- when every instance/observation in the subset belongs to the same class (we can't make it more pure)\n",
    "- there are no more attributes to be selected: A classic rule when the leaves are not 100% pure (i.e. If examples still do not belong to the same class) is to assign/label the leaf node with the most common class of the examples in the subset. E.g. leaf node has 3 positive and 6 negative labels, then we will consider the leaf node to be labeled as negative. And all new observations that will fall into that leaf node will be predicted as negative.\n",
    "- There are no instance in the subset (empty leaf node)\n",
    "- (optional) when the error measure associated with a node falls below a certain tolerance level\n",
    "- (optional) when the error reduction $\\Delta E$ resulting from further splitting does not exceed a threshold value.\n",
    "\n",
    "In machine learning the most representative methods of decision-tree induction are the **ID3** and the **C4 algorithms**. Similar techniques were introduced in statistics by Breiman et al., whose methodology is often referred to as the **CART (Classification and Regression Trees) algorithm**.\n",
    "\n",
    "***ID3 Algorithm***: famous Decision Tree algorithm:  \n",
    "  1. Calculate entropy of every attribute using training data set S\n",
    "  2. Split the set S into subsets using the attribute for which the resulting entropy (after splitting) is minimum (or, equivalently, information gain is maximum)  \n",
    "  3. Make a decision tree node containing that attribute  \n",
    "  4. Recurse on subsets using remaining attributes.  \n",
    "\n",
    "However, the ID3 does not guarantee the optimal solution: \n",
    "- local minima: It will find the local minima but does not guarantee that local minima is a global minima!  \n",
    "- Overfit: it can also overfit the data\n",
    "  Decision tree learning aims at minimising the impurity. And therefore it has a tendency to build large and complex trees. It also tries to perfectly fit the training data. As a result, it might overfit the training data, which implies that it fails to generalise (unable to correctly predict new observations). Therefore, it is important to find the *sweet spot* between high accuracy and small tree size (nb of nodes).\n",
    "  \n",
    "In order to solve eventual **overfitting issues**, there are some best practices:\n",
    "- Limit the length of the tree\n",
    "- Limit the number of leaves\n",
    "- Stop growing when the split is not statistically significant \n",
    "- Grow full tree and then prune subtrees\n",
    "\n",
    "***C4.5 Algorithm***: improvement of ID3, some characteristics:\n",
    "- able to handle **both continuous and discrete variables/attributes**. This is done by creating splits based on thresholds (e.g. Price > 50). It is done by using the Gain Ratio, a variant of the Information Gain.\n",
    "- Handles missing values: they're simply not computed in entropy computation\n",
    "- Prune the tree after creation: \n",
    "    - Remove subtrees that do not significantly help \n",
    "    - Error-based pruning\n",
    "\n",
    "\n",
    "- Advantages:\n",
    "    - non-linearity\n",
    "    - support for categorical variables\n",
    "    - easy to interpret\n",
    "    - application to regression tasks (see below)\n",
    "    - no feature engineering ???\n",
    "    \n",
    "- Disadvantages:\n",
    "    - Prone to overfitting (low bias but high variance --> poor generalisation)  \n",
    "    Dividing the data by partitioning the input space shows typically small estimator bias but at the cost of increased variance. This is particularly problematic in high-dimensional spaces where data become sparse. Some solutions exist:  \n",
    "        - One response to the problem is the **adoption of simple local models (e.g. constant or linear)**. These simple functions minimise the variance at the cost of an increased bias.\n",
    "        - Another trick is to make use of **soft splits**, allowing data to lie simultaneously in multiple regions. This is the approach taken by BFN (Basis Function Networks).\n",
    "    - instable (not robust to noise)\n",
    "    \n",
    "#### Decision Tree Hyperparameters\n",
    "\n",
    "Bias/variance trade-off is managed by a number of hyperparameters:\n",
    "- Minimum number of samples per leaf\n",
    "- Maximum number of leaves\n",
    "- Maximum depth\n",
    "- Growing criterion (not simply training error)   \n",
    "- Shrinking parameter $\\lambda$\n",
    "\n",
    "#### Decision Trees for Regression\n",
    "\n",
    "(the ML handbook focuses on regression trees! so formulas are available there)\n",
    "Above we presented how Decision Trees work for classification tasks. But Decision Trees can also be used for regression tasks, after some modifications. \n",
    "\n",
    "The CART uses variance to choose the best attribute. In the regression case, instead of assigning the most common occurences in a leaf to be the class of the prediction (e.g. 6 positive and 2 negative then we will assign the prediction to the positive class as it is the most common in the leaf), we can use the average in the leaves to predict the value.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "An animated version of decision trees can be found here : http://www.r2d3.us/visual-intro-to-machine-learning-part-1/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select variables for the model\n",
    "X = encoded_train_set[['Age', 'SibSp', 'Parch', 'Fare', 'Embarked_Q', 'Embarked_S', 'Pclass_2', 'Pclass_3', 'Sex_male']]\n",
    "y = encoded_train_set[\"Survived\"]\n",
    "\n",
    "# create model\n",
    "#model = LogisticRegression(solver='lbfgs', max_iter=200)\n",
    "sktree = DecisionTreeClassifier() # default parameters\n",
    "\n",
    "# prepare the cross-validation procedure\n",
    "cv_10 = KFold(n_splits=10, random_state=1, shuffle=True)\n",
    "\n",
    "# evaluate model\n",
    "#scores = cross_val_score(model, X=X, y=y, scoring='accuracy', cv=cv_10, n_jobs=-1)\n",
    "scores = cross_val_score(sktree, X=X, y=y, scoring='accuracy', cv=cv_10, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.792 (std dev: 0.034)\n"
     ]
    }
   ],
   "source": [
    "# Performance\n",
    "print('Accuracy: %.3f (std dev: %.3f)' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume we're happy with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're happy with the model, let's predict on the encoded_test_set that contains only the X (and y has to be predicted)\n",
    "\n",
    "X_test = encoded_test_set[['PassengerId','Age', 'SibSp', 'Parch', 'Fare', 'Embarked_Q', 'Embarked_S', 'Pclass_2', 'Pclass_3', 'Sex_male']]\n",
    "\n",
    "\n",
    "# Input data for the training phase\n",
    "X_train = encoded_train_set[['Age', 'SibSp', 'Parch', 'Fare', 'Embarked_Q', 'Embarked_S', 'Pclass_2', 'Pclass_3', 'Sex_male']]\n",
    "y_train = encoded_train_set[\"Survived\"]\n",
    "\n",
    "#create model\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "\n",
    "#fit the model on the whole dataset\n",
    "decision_tree.fit(X=X_train , y=y_train) \n",
    "\n",
    "# use fitted model to predict given input X_test\n",
    "prediction = decision_tree.predict(X_test.loc[ : , X_test.columns != 'PassengerId']) #use X_test except the PassengerId column to predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived\n",
       "0          892         0\n",
       "1          893         0\n",
       "2          894         1\n",
       "3          895         1\n",
       "4          896         1"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate the prediction with the PassengerId\n",
    "\n",
    "#Apparently this is the way to do it, I hope rows stay consistent\n",
    "# https://stackoverflow.com/questions/43549034/map-predictions-back-to-ids-python-scikit-learn-decisiontreeclassifier\n",
    "# https://stackoverflow.com/questions/51078742/recovering-instances-ids-after-training-and-prediction\n",
    "\n",
    "submission = pd.DataFrame(data=X_test[\"PassengerId\"])   # We take (the PassengerId column of) X_test\n",
    "submission['Survived'] = prediction                   # We add a column called 'Survived' (as asked on Kaggle)\n",
    "submission.head() #Shows the first few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deliver the output\n",
    "submission.to_csv(\"/Users/guillaumedelande/Documents/Programming/Data Science - Engineering/Training/Titanic dataset/basic_tree.csv\",index=False) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select variables for the model\n",
    "X = encoded_train_set[['Age', 'SibSp', 'Parch', 'Fare', 'Embarked_Q', 'Embarked_S', 'Pclass_2', 'Pclass_3', 'Sex_male']]\n",
    "y = encoded_train_set[\"Survived\"]\n",
    "\n",
    "# create model\n",
    "#model = LogisticRegression(solver='lbfgs', max_iter=200)\n",
    "sktree = DecisionTreeClassifier() # default parameters\n",
    "\n",
    "# prepare the cross-validation procedure\n",
    "cv_10 = KFold(n_splits=10, random_state=1, shuffle=True)\n",
    "\n",
    "# evaluate model\n",
    "#scores = cross_val_score(model, X=X, y=y, scoring='accuracy', cv=cv_10, n_jobs=-1)\n",
    "scores = cross_val_score(sktree, X=X, y=y, scoring='accuracy', cv=cv_10, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance\n",
    "print('Accuracy: %.3f (std dev: %.3f)' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "We choose the random forest for the following reason:\n",
    "- Non-linear\n",
    "- Embedded feature selection\n",
    "- Easy handling of categorical features\n",
    "- +- Interpretable (thanks to the importance of a given variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic model (no tuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters tuning\n",
    "\n",
    "The random forest has mainly two hyperparameters: the number of variables randomly selected for each tree (mtry) & the number of trees. We can also limit the depth of each tree, but this is often linked to the number of variables. \n",
    "\n",
    "\n",
    "In general, adding more trees is beneficial to the prediction, however having more variables for each tree might lead to longer computation without increase in performance. It might also increase the variance of the classifier because it might increase the correlation between the trees (more or less the same each time). It may increase the risk of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression: OK\n",
    "## Decision Trees: OK needs tuning\n",
    "## Support Vector Machines (SVM)\n",
    "\n",
    "## GBM\n",
    "## XXX\n",
    "\n",
    "\n",
    "\n",
    "In machine learning plenty of algorithms for nonlinear regression has been proposed. Here it is just a small subset\n",
    "  Feedforward neural network, Deep learning\n",
    "  Regression tree, Random Forest, Gradient boosting trees\n",
    "  Neuro-fuzzy inference systems\n",
    "  Radial basis function\n",
    "  Local modeling regression\n",
    "  Kernel methods\n",
    "  Support vector machine\n",
    "All\n",
    "  rely (implicitly or explicitly) on a different set of assumptions (also known as inductive bias) about the target function.\n",
    "  have a set of (hyper)parameters which control the bias/variance trade-off (like knobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "We've seen 10-fold cross-validation for:\n",
    "- Logistic Regression: 80% accuracy - Kaggle 75%\n",
    "- Basic Decision Tree: 79% accuracy - Kaggle 71%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://)### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Descriptive Analysis: studying Interdependencies\n",
    "\n",
    "In this part we study the correlations between the categorical Y variable (i.e. Survived) and the explanatory variables (X). The explanatory variables can take 2 different forms: categorical or continuous. For each type, a different method is used to study the correlation with the Y categorical variable, and with the other variables.\n",
    "\n",
    "\n",
    "\n",
    "### Correlations Cat(Y) - Cat(X)\n",
    "\n",
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining factors (for categorical variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
